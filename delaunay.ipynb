{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import Delaunay\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_grid = np.meshgrid(np.arange(0, 129, 8), np.arange(0, 129, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "coors = np.concatenate([mesh_grid[0][..., np.newaxis], mesh_grid[1][..., np.newaxis]], axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "coors = coors.reshape(-1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = coors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tris = Delaunay(coors)\n",
    "plt.triplot(points[:, 0], points[:, 1], tris.simplices.copy(), c='black')\n",
    "plt.plot(points[:, 0], points[:, 1], 'o', c='green')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tris.simplices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.solve_DLT import *\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_flow = torch.tensor([(0, 0), (0, 0), (0, 0), (0, 0), (-100, -100), (0, 0), (0, 0), (0, 0), (0, 0)], dtype=torch.float, device=torch.device(\"cpu\")).permute(1, 0).reshape(2, 3, 3).unsqueeze(0)\n",
    "mesh_flow = mesh_flow_upsampling(mesh_flow, (3, 3), (9, 9), (1280, 1280), 1, torch.device(\"cpu\"))\n",
    "# b, c = solve_mesh_flow_DLT_triangle(mesh_flow, torch.device(\"cpu\"), (160, 160), (1280, 1280))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coors = mesh_flow.squeeze(0).permute(1, 2, 0).numpy()\n",
    "print(coors.shape)\n",
    "plt.figure()\n",
    "plt.scatter(coors[:, 0],coors[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(origin_mesh_grid.shape)\n",
    "print(mesh_flow.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = (1280, 1280)\n",
    "patch_size = (160, 160)\n",
    "device = torch.device(\"cpu\")\n",
    "from scipy.spatial import Delaunay\n",
    "batch_size = mesh_flow.shape[0]\n",
    "mesh_grid_X, mesh_grid_Y = torch.meshgrid(torch.arange(start=0, end=image_size[0] + 1, step=patch_size[0], dtype=torch.float, device=device),\n",
    "    torch.arange(start=0, end=image_size[1] + 1, step=patch_size[1], dtype=torch.float, device=device,),\n",
    "    indexing=\"ij\",\n",
    ")\n",
    "mesh_grid = torch.cat([mesh_grid_X.unsqueeze(0), mesh_grid_Y.unsqueeze(0)], dim=0)\n",
    "mesh_grid_flat = mesh_grid.permute(1, 2, 0).reshape(-1, 2)\n",
    "tris = Delaunay(mesh_grid_flat.cpu().numpy())\n",
    "tris_samples_indices = torch.from_numpy(tris.simplices).type(torch.int64)\n",
    "origin_tris_samples = mesh_grid_flat[tris_samples_indices].unsqueeze(0).expand(batch_size, -1, -1, -1)\n",
    "origin_mesh_grid = torch.cat([mesh_grid_X.unsqueeze(0), mesh_grid_Y.unsqueeze(0)], dim=0).unsqueeze(0).expand(batch_size, -1, -1, -1)\n",
    "warped_mesh_grid = (origin_mesh_grid + mesh_flow).permute(0, 2, 3, 1).reshape(batch_size, -1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warped_tris_samples = warped_mesh_grid[:, tris_samples_indices, :]\n",
    "solved_matrices = solve_affine_DLT(warped_tris_samples.reshape(-1, 3, 2), origin_tris_samples.reshape(-1, 3, 2), device=device)\n",
    "\n",
    "points_grid = torch.meshgrid(torch.arange(start=0.5, end=image_size[0], step=1, dtype=torch.float, device=device), \\\n",
    "    torch.arange(start=0.5, end=image_size[1], step=1, dtype=torch.float, device=device), indexing='xy')\n",
    "points_grid = torch.cat([points_grid[0].unsqueeze(2), points_grid[1].unsqueeze(2)], dim=2).unsqueeze(0).expand(origin_tris_samples.reshape(-1, 3, 2).shape[0], -1, -1, -1)\n",
    "\n",
    "def GetCross(point1, point2, points_grid, mesh_grid):\n",
    "    cross_product = (mesh_grid[:, 0:1, point2:point2 + 1, 0] - mesh_grid[:, 0:1, point1:point1 + 1, 0]) * (points_grid[:, :, :, 1] - mesh_grid[:, 0:1, point1:point1 + 1, 1]) - \\\n",
    "            (points_grid[:, :, :, 0] - mesh_grid[:, 0:1, point1:point1 + 1, 0]) * (mesh_grid[:, 0:1, point2:point2 + 1, 1] - mesh_grid[:, 0:1, point1:point1 + 1, 1])\n",
    "    return cross_product\n",
    "\n",
    "warped_tris_samples = warped_tris_samples.reshape(-1, 1, 3, 2)\n",
    "cross12 = GetCross(0, 1, points_grid, warped_tris_samples)\n",
    "cross23 = GetCross(1, 2, points_grid, warped_tris_samples)\n",
    "cross31 = GetCross(2, 0, points_grid, warped_tris_samples)\n",
    "points_grid_mask = torch.bitwise_and((cross12 * cross23) >= 0, (cross23 * cross31) >= 0,).float()\n",
    "# For Test\n",
    "print(points_grid_mask.shape)\n",
    "mask_value = (torch.arange(0, 128, 1, device=device) * 2).unsqueeze(1).unsqueeze(2)\n",
    "tris_map = (mask_value * points_grid_mask).sum(dim=0)\n",
    "im = transforms.ToPILImage()(tris_map)\n",
    "im.save(\"tris_map.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.arange(12).reshape(2, 3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.flip(a, dims=(2,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warped_mesh_grid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coors = warped_mesh_grid.squeeze(0).numpy()\n",
    "print(coors.shape)\n",
    "plt.figure()\n",
    "plt.scatter(coors[:, 1],coors[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_mesh_flow_DLT_triangle(mesh_flow: torch.Tensor, device: torch.device, patch_size: Tuple[int], image_size: Tuple[int]) -> Tuple[torch.Tensor]:\n",
    "    # TODO: In future development, Delaunay mesh calculation should be completed outside the function\n",
    "    from scipy.spatial import Delaunay\n",
    "    batch_size = mesh_flow.shape[0]\n",
    "    mesh_grid_X, mesh_grid_Y = torch.meshgrid(torch.arange(start=0, end=image_size[0] + 1, step=patch_size[0], dtype=torch.float, device=device),\n",
    "        torch.arange(start=0, end=image_size[1] + 1, step=patch_size[1], dtype=torch.float, device=device,),\n",
    "        indexing=\"xy\",\n",
    "    )\n",
    "    mesh_grid = torch.cat([mesh_grid_X.unsqueeze(0), mesh_grid_Y.unsqueeze(0)], dim=0)\n",
    "    mesh_grid_flat = mesh_grid.permute(1, 2, 0).reshape(-1, 2)\n",
    "    tris = Delaunay(mesh_grid_flat.cpu().numpy())\n",
    "    tris_samples_indices = torch.from_numpy(tris.simplices).type(torch.int64)\n",
    "    origin_tris_samples = mesh_grid_flat[tris_samples_indices].unsqueeze(0).expand(batch_size, -1, -1, -1)\n",
    "    origin_mesh_grid = torch.cat([mesh_grid_X.unsqueeze(0), mesh_grid_Y.unsqueeze(0)], dim=0).unsqueeze(0).expand(batch_size, -1, -1, -1)\n",
    "    warped_mesh_grid = (origin_mesh_grid + mesh_flow).permute(0, 2, 3, 1).reshape(batch_size, -1, 2)\n",
    "    warped_tris_samples = warped_mesh_grid[:, tris_samples_indices, :]\n",
    "    solved_matrices = solve_affine_DLT(warped_tris_samples.reshape(-1, 3, 2), origin_tris_samples.reshape(-1, 3, 2), device=device)\n",
    "\n",
    "    points_grid = torch.meshgrid(torch.arange(start=0.5, end=image_size[0], step=1, dtype=torch.float, device=device), \\\n",
    "        torch.arange(start=0.5, end=image_size[1], step=1, dtype=torch.float, device=device), indexing='xy')\n",
    "    points_grid = torch.cat([points_grid[0].unsqueeze(2), points_grid[1].unsqueeze(2)], dim=2).unsqueeze(0).expand(origin_tris_samples.reshape(-1, 3, 2).shape[0], -1, -1, -1)\n",
    "\n",
    "    def GetCross(point1, point2, points_grid, mesh_grid):\n",
    "        cross_product = (mesh_grid[:, 0:1, point2:point2 + 1, 0] - mesh_grid[:, 0:1, point1:point1 + 1, 0]) * (points_grid[:, :, :, 1] - mesh_grid[:, 0:1, point1:point1 + 1, 1]) - \\\n",
    "                (points_grid[:, :, :, 0] - mesh_grid[:, 0:1, point1:point1 + 1, 0]) * (mesh_grid[:, 0:1, point2:point2 + 1, 1] - mesh_grid[:, 0:1, point1:point1 + 1, 1])\n",
    "        return cross_product\n",
    "\n",
    "    warped_tris_samples = warped_tris_samples.reshape(-1, 1, 3, 2)\n",
    "    cross12 = GetCross(0, 1, points_grid, warped_tris_samples)\n",
    "    cross23 = GetCross(1, 2, points_grid, warped_tris_samples)\n",
    "    cross31 = GetCross(2, 0, points_grid, warped_tris_samples)\n",
    "    points_grid_mask = torch.bitwise_and((cross12 * cross23) >= 0, (cross23 * cross31) >= 0,).float()\n",
    "    # For Test\n",
    "    print(points_grid_mask.shape)\n",
    "    mask_value = (torch.arange(0, 128, 1, device=device) * 2).unsqueeze(1).unsqueeze(2)\n",
    "    tris_map = (mask_value * points_grid_mask).sum(dim=0)\n",
    "    im = transforms.ToPILImage()(tris_map)\n",
    "    im.save(\"tris_map.jpg\")\n",
    "    points_grid = points_grid.reshape(-1, image_size[0] * image_size[1], 2)\n",
    "    # print(solved_matrices.shape, points_grid.shape)\n",
    "    warped_points_grid = torchgeometry.core.transform_points(solved_matrices, points_grid).reshape(batch_size, -1, image_size[0], image_size[1], 2)\n",
    "    points_grid_mask = points_grid_mask.reshape(batch_size, -1, image_size[0], image_size[1], 1)\n",
    "    warped_points = (warped_points_grid * points_grid_mask).sum(dim=1) / points_grid_mask.sum(dim=1)\n",
    "    # torch.save(points_grid_mask.sum(dim=1), \"test.pt\")\n",
    "    warped_points = warped_points.permute(0, 3, 1, 2)\n",
    "\n",
    "    return warped_points, solved_matrices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchgeometry\n",
    "import torch.nn as nn\n",
    "from typing import *\n",
    "from torchgeometry.core.transformations import transform_points\n",
    "\n",
    "\n",
    "def mesh_flow_upsampling(mesh_flow_tensor:torch.Tensor, mesh_grid_size:Tuple[int], upsample_grid_size:Tuple[int], img_size:Tuple[int], batch_size:int, device:torch.device):\n",
    "    \"\"\"upsample mesh flow result from mesh grid size to upsampling grid size\n",
    "\n",
    "    Args:\n",
    "      mesh_flow_tensor: tensor result of mesh flow. Data format: (N, 2, mesh_grid_size[0], mesh_grid_size[1])\n",
    "      mesh_grid_size: mesh grid of low resolution\n",
    "      upsample_grid_size: mesh grid of high resolutin\n",
    "      img_size: size of input image\n",
    "      batch_size: batch size of data\n",
    "      device: torch device (cpu or gpu)\n",
    "    \n",
    "    Returns:\n",
    "      upsampled mesh flow tensor with shape: (N, 2, upsample_grid_size[0], upsample_grid_size[1])\n",
    "    \"\"\"\n",
    "    # step 1: unfold mesh flow tensor\n",
    "    unfold = nn.Unfold(kernel_size=(2, 2))\n",
    "    # caution: unfold tensor data format is [batch_size, c * kernel_h * kernel_w, patches]\n",
    "    mesh_flow = unfold(mesh_flow_tensor).permute(0, 2, 1)\n",
    "    mesh_flow = mesh_flow.reshape(mesh_flow.shape[0] * mesh_flow.shape[1], -1)\n",
    "    mesh_flow = mesh_flow.reshape(-1, 2, 4).permute(0, 2, 1)\n",
    "    # step 2: get mesh grid of mesh_grid size\n",
    "    y_t = torch.matmul(torch.ones(mesh_grid_size[0], 1, dtype=torch.float, device=device), \\\n",
    "      torch.linspace(0, img_size[1], mesh_grid_size[1], dtype=torch.float, device=device).unsqueeze(0)).unsqueeze(2)\n",
    "    x_t = torch.matmul(torch.linspace(0, img_size[0], mesh_grid_size[0], dtype=torch.float, device=device).unsqueeze(1), \\\n",
    "      torch.ones(1, mesh_grid_size[1], dtype=torch.float, device=device)).unsqueeze(2)\n",
    "    # grid_sparse shape: [batch_size, 2, mesh_grid_size[0], mesh_grid_size[1]]\n",
    "    grid_sparse = torch.cat([y_t, x_t], dim=2).permute(2, 0, 1).unsqueeze(0).expand(batch_size, -1, -1, -1)\n",
    "    # step 3: unfold sparse grid\n",
    "    # unfolded grid sparse shape: [batch_size, 2 * 2 * 2, (mesh_grid_size[0] - 1) * (mesh_grid_size[1] - 1)]\n",
    "    grid_sparse_unfold = unfold(grid_sparse).permute(0, 2, 1)\n",
    "    # unfolded grid sparse shape: [batch_size * (mesh_grid_size[0] - 1) * (mesh_grid_size[1] - 1), 2 * 2 * 2]\n",
    "    grid_sparse_unfold = grid_sparse_unfold.reshape(grid_sparse_unfold.shape[0] * grid_sparse_unfold.shape[1], -1)\n",
    "    # unfolded grid sparse shape: [batch_size * (mesh_grid_size[0] - 1) * (mesh_grid_size[1] - 1), 4, 2]\n",
    "    grid_sparse_unfold = grid_sparse_unfold.reshape(-1, 2, 4).permute(0, 2, 1)\n",
    "    # step 4: use original mesh grid and mesh flow to calculate mesh upsampling homography matrices\n",
    "    solved_matrices = torchgeometry.get_perspective_transform(grid_sparse_unfold, grid_sparse_unfold + mesh_flow)\n",
    "    # print(grid_sparse_unfold[1], \"\\n\", (grid_sparse_unfold + mesh_flow)[1])\n",
    "\n",
    "    # Whether Scaling?\n",
    "    # patch_width = img_size[1] / (mesh_grid_size[1] - 1)\n",
    "    # patch_height = img_size[0] / (mesh_grid_size[0] - 1)\n",
    "    # solved_matrices_scaled = H_scale(solved_matrices, patch_width=patch_width, patch_height=patch_height, batch_size=batch_size)\n",
    "\n",
    "    # step 5: get dense grid of upsample_grid_size\n",
    "    y_t = torch.matmul(torch.ones(upsample_grid_size[0], 1, dtype=torch.float, device=device), \\\n",
    "      torch.linspace(0, img_size[1], upsample_grid_size[1], dtype=torch.float, device=device).unsqueeze(0)).unsqueeze(2)\n",
    "    x_t = torch.matmul(torch.linspace(0, img_size[0], upsample_grid_size[0], dtype=torch.float, device=device).unsqueeze(1), \\\n",
    "      torch.ones(1, upsample_grid_size[1], dtype=torch.float, device=device)).unsqueeze(2)\n",
    "    # grid shape: [batch_size, 2, upsample_grid_size[0], upsample_grid_size[1]]\n",
    "    grid_dense = torch.cat([y_t, x_t], dim=2).permute(2, 0, 1).unsqueeze(0).expand(batch_size, -1, -1, -1)\n",
    "    # step 6: unfold dense grid\n",
    "    unfold_kernel_size = ((upsample_grid_size[0] - 1) // (mesh_grid_size[0] - 1) + 1, (upsample_grid_size[1] - 1) // (mesh_grid_size[1] - 1) + 1)\n",
    "    unfold_stride = ((upsample_grid_size[0] - 1) // (mesh_grid_size[0] - 1), (upsample_grid_size[1] - 1) // (mesh_grid_size[1] - 1))\n",
    "    unfold2 = nn.Unfold(kernel_size=unfold_kernel_size, stride=unfold_stride)\n",
    "    # unfold_grid_dense (direct result) shape: [batch_size, 2 * unfold_kernel_size[0]*[1], patches]\n",
    "    unfold_grid_dense = unfold2(grid_dense).permute(0, 2, 1).reshape(solved_matrices.shape[0], -1)\n",
    "    unfold_grid_dense = unfold_grid_dense.reshape(-1, 2, unfold_kernel_size[0] * unfold_kernel_size[1]).permute(0, 2, 1)\n",
    "    # step 7: transform points in unfolded dense grid using homography matrices solved from sparse mesh\n",
    "    warped_unfold_grid_dense = transform_points(solved_matrices, unfold_grid_dense)\n",
    "    # print(unfold_grid_dense[1])\n",
    "    # print(warped_unfold_grid_dense[1])\n",
    "    # print((warped_unfold_grid_dense[1] - unfold_grid_dense[1]))\n",
    "    warped_unfold_grid_dense = warped_unfold_grid_dense.reshape(batch_size, -1, unfold_kernel_size[0], unfold_kernel_size[1], 2)\n",
    "    warped_unfold_grid_dense = warped_unfold_grid_dense.permute(0, 4, 2, 3, 1).reshape(batch_size, 2 * unfold_kernel_size[0] * unfold_kernel_size[1], -1)\n",
    "    \n",
    "    # Caution: fold is not the inverse operation of unfold. It will sum the values of overlapped region\n",
    "    # step 8: fold back transform points to mesh grid\n",
    "    fold = torch.nn.Fold(output_size = (upsample_grid_size[0], upsample_grid_size[1]), kernel_size=unfold_kernel_size, stride=unfold_stride)\n",
    "    warped_grid_dense = fold(warped_unfold_grid_dense)\n",
    "    X_border = torch.arange(start=0, end=upsample_grid_size[0] - 1, step=unfold_kernel_size[0] - 1, dtype=torch.long)[1:]\n",
    "    Y_border = torch.arange(start=0, end=upsample_grid_size[1] - 1, step=unfold_kernel_size[1] - 1, dtype=torch.long)[1:]\n",
    "    X1, Y1 = torch.meshgrid(torch.arange(upsample_grid_size[0], dtype=torch.long), Y_border, indexing=\"ij\")\n",
    "    X2, Y2 = torch.meshgrid(X_border, torch.arange(upsample_grid_size[1], dtype=torch.long), indexing=\"ij\")\n",
    "    warped_grid_dense[:, :, X1, Y1] /= 2\n",
    "    warped_grid_dense[:, :, X2, Y2] /= 2\n",
    "\n",
    "    # step 9: subtract origin dense mesh grid from transformed mesh grid to obtain upsampled mesh flow\n",
    "    upsample_mesh_flow = warped_grid_dense - grid_dense + grid_dense\n",
    "    out = upsample_mesh_flow\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_grid_size = (3, 3)\n",
    "img_size = (128, 128)\n",
    "y_t = torch.matmul(torch.ones(mesh_grid_size[0], 1, dtype=torch.float, device=device), \\\n",
    "      torch.linspace(0, img_size[1], mesh_grid_size[1], dtype=torch.float, device=device).unsqueeze(0)).unsqueeze(2)\n",
    "x_t = torch.matmul(torch.linspace(0, img_size[0], mesh_grid_size[0], dtype=torch.float, device=device).unsqueeze(1), \\\n",
    "      torch.ones(1, mesh_grid_size[1], dtype=torch.float, device=device)).unsqueeze(2)\n",
    "    # grid_sparse shape: [batch_size, 2, mesh_grid_size[0], mesh_grid_size[1]]\n",
    "grid_sparse = torch.cat([y_t, x_t], dim=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulate Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-08-12 16:23:13,306][367716492.py][line:27][INFO] Parameters Setting:\n",
      "[2022-08-12 16:23:13,306][367716492.py][line:27][INFO] Parameters Setting:\n",
      "[2022-08-12 16:23:13,308][367716492.py][line:29][INFO] gpu: 1\n",
      "[2022-08-12 16:23:13,308][367716492.py][line:29][INFO] gpu: 1\n",
      "[2022-08-12 16:23:13,310][367716492.py][line:29][INFO] cpu: False\n",
      "[2022-08-12 16:23:13,310][367716492.py][line:29][INFO] cpu: False\n",
      "[2022-08-12 16:23:13,312][367716492.py][line:29][INFO] seed: 1\n",
      "[2022-08-12 16:23:13,312][367716492.py][line:29][INFO] seed: 1\n",
      "[2022-08-12 16:23:13,314][367716492.py][line:29][INFO] exp_name: test\n",
      "[2022-08-12 16:23:13,314][367716492.py][line:29][INFO] exp_name: test\n",
      "[2022-08-12 16:23:13,316][367716492.py][line:29][INFO] model_name: test\n",
      "[2022-08-12 16:23:13,316][367716492.py][line:29][INFO] model_name: test\n",
      "[2022-08-12 16:23:13,317][367716492.py][line:29][INFO] resume: False\n",
      "[2022-08-12 16:23:13,317][367716492.py][line:29][INFO] resume: False\n",
      "[2022-08-12 16:23:13,320][367716492.py][line:29][INFO] num_workers: 8\n",
      "[2022-08-12 16:23:13,320][367716492.py][line:29][INFO] num_workers: 8\n",
      "[2022-08-12 16:23:13,321][367716492.py][line:29][INFO] config: config.yaml\n",
      "[2022-08-12 16:23:13,321][367716492.py][line:29][INFO] config: config.yaml\n",
      "[2022-08-12 16:23:13,323][367716492.py][line:29][INFO] stage: train\n",
      "[2022-08-12 16:23:13,323][367716492.py][line:29][INFO] stage: train\n",
      "[2022-08-12 16:23:13,325][367716492.py][line:29][INFO] image_size: (128, 128)\n",
      "[2022-08-12 16:23:13,325][367716492.py][line:29][INFO] image_size: (128, 128)\n",
      "[2022-08-12 16:23:13,326][367716492.py][line:29][INFO] mesh_size_1: (2, 2)\n",
      "[2022-08-12 16:23:13,326][367716492.py][line:29][INFO] mesh_size_1: (2, 2)\n",
      "[2022-08-12 16:23:13,328][367716492.py][line:29][INFO] mesh_size_2: (5, 5)\n",
      "[2022-08-12 16:23:13,328][367716492.py][line:29][INFO] mesh_size_2: (5, 5)\n",
      "[2022-08-12 16:23:13,330][367716492.py][line:29][INFO] mesh_size_3: (17, 17)\n",
      "[2022-08-12 16:23:13,330][367716492.py][line:29][INFO] mesh_size_3: (17, 17)\n",
      "[2022-08-12 16:23:13,331][367716492.py][line:29][INFO] epoch: 100\n",
      "[2022-08-12 16:23:13,331][367716492.py][line:29][INFO] epoch: 100\n",
      "[2022-08-12 16:23:13,333][367716492.py][line:29][INFO] batch_size: 8\n",
      "[2022-08-12 16:23:13,333][367716492.py][line:29][INFO] batch_size: 8\n",
      "[2022-08-12 16:23:13,334][367716492.py][line:29][INFO] lr: 0.0001\n",
      "[2022-08-12 16:23:13,334][367716492.py][line:29][INFO] lr: 0.0001\n",
      "[2022-08-12 16:23:13,335][367716492.py][line:29][INFO] weight_decay: 0.96\n",
      "[2022-08-12 16:23:13,335][367716492.py][line:29][INFO] weight_decay: 0.96\n",
      "[2022-08-12 16:23:13,337][367716492.py][line:29][INFO] print_every_iter: 50\n",
      "[2022-08-12 16:23:13,337][367716492.py][line:29][INFO] print_every_iter: 50\n",
      "[2022-08-12 16:23:13,338][367716492.py][line:29][INFO] record_every_iter: 250\n",
      "[2022-08-12 16:23:13,338][367716492.py][line:29][INFO] record_every_iter: 250\n",
      "[2022-08-12 16:23:13,339][367716492.py][line:29][INFO] save_every_epoch: 30\n",
      "[2022-08-12 16:23:13,339][367716492.py][line:29][INFO] save_every_epoch: 30\n",
      "[2022-08-12 16:23:13,340][367716492.py][line:29][INFO] share: 1\n",
      "[2022-08-12 16:23:13,340][367716492.py][line:29][INFO] share: 1\n",
      "[2022-08-12 16:23:13,342][367716492.py][line:29][INFO] train_data: /home/wyq/DeepImageStitching-pytorch/align_data/train/\n",
      "[2022-08-12 16:23:13,342][367716492.py][line:29][INFO] train_data: /home/wyq/DeepImageStitching-pytorch/align_data/train/\n",
      "[2022-08-12 16:23:13,344][367716492.py][line:29][INFO] test_data: /home/wyq/DeepImageStitching-pytorch/align_data/test/\n",
      "[2022-08-12 16:23:13,344][367716492.py][line:29][INFO] test_data: /home/wyq/DeepImageStitching-pytorch/align_data/test/\n",
      "[2022-08-12 16:23:13,346][367716492.py][line:29][INFO] data_inverse: 0\n",
      "[2022-08-12 16:23:13,346][367716492.py][line:29][INFO] data_inverse: 0\n",
      "[2022-08-12 16:23:13,348][367716492.py][line:29][INFO] data_random_inverse: 0\n",
      "[2022-08-12 16:23:13,348][367716492.py][line:29][INFO] data_random_inverse: 0\n",
      "[2022-08-12 16:23:13,351][367716492.py][line:29][INFO] brightness: 0.2\n",
      "[2022-08-12 16:23:13,351][367716492.py][line:29][INFO] brightness: 0.2\n",
      "[2022-08-12 16:23:13,352][367716492.py][line:29][INFO] contrast: 0.2\n",
      "[2022-08-12 16:23:13,352][367716492.py][line:29][INFO] contrast: 0.2\n",
      "[2022-08-12 16:23:13,354][367716492.py][line:29][INFO] saturation: 0.2\n",
      "[2022-08-12 16:23:13,354][367716492.py][line:29][INFO] saturation: 0.2\n",
      "[2022-08-12 16:23:13,356][367716492.py][line:29][INFO] loss_weight_lambda: 0.5\n",
      "[2022-08-12 16:23:13,356][367716492.py][line:29][INFO] loss_weight_lambda: 0.5\n",
      "[2022-08-12 16:23:13,357][367716492.py][line:29][INFO] loss_weight_mu: 0.01\n",
      "[2022-08-12 16:23:13,357][367716492.py][line:29][INFO] loss_weight_mu: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-08-12 16:24:52,808][367716492.py][line:91][INFO] Epoch 1 | psnr 1 10.729942321777344 | ssim 1 0.2526123821735382 | psnr 2 11.643092155456543 | ssim 2 0.23057489097118378 |\n",
      "[2022-08-12 16:24:52,808][367716492.py][line:91][INFO] Epoch 1 | psnr 1 10.729942321777344 | ssim 1 0.2526123821735382 | psnr 2 11.643092155456543 | ssim 2 0.23057489097118378 |\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from dataset import align_dataset\n",
    "\n",
    "from config.option import args\n",
    "from utils.get_logger import Get_logger\n",
    "from utils.metrics import calc_psnr\n",
    "from utils.solve_DLT import solve_mesh_flow_DLT, spatial_transform_by_grid\n",
    "from utils.toolkit import tensor2img\n",
    "from pytorch_msssim.ssim import ssim\n",
    "from models.deep_mesh_flow import DeepMeshFlow\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "if args.cpu:\n",
    "    device = torch.device(\"cpu\")\n",
    "else:\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu\n",
    "    device = torch.device(\"cuda:0\")    \n",
    "\n",
    "# initialize logging file\n",
    "logger = Get_logger(os.path.join(\"log\", args.exp_name + \".log\"))\n",
    "logger.info(\"Parameters Setting:\")\n",
    "for i in args.__dict__:\n",
    "    logger.info(\"{param_name}: {param_value}\".format(param_name=i, param_value=args.__dict__[i]))\n",
    "\n",
    "model = DeepMeshFlow(args=args, device=device)\n",
    "model = model.to(device)\n",
    "train_dataset = align_dataset(args=args)\n",
    "val_dataset = align_dataset(args=args, validation=True)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.num_workers)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.num_workers)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer=optimizer, step_size=1, gamma=args.weight_decay)\n",
    "\n",
    "# result save\n",
    "model.load_state_dict(torch.load(\"checkpoints/test/align_latest.pth\")['state_dict'])\n",
    "model.eval()\n",
    "ssim_result1 = []\n",
    "ssim_result2 = []\n",
    "psnr_result1 = []\n",
    "psnr_result2 = []\n",
    "with torch.no_grad():\n",
    "    for i, sample_batch in enumerate(val_loader):\n",
    "        print(i)\n",
    "        input_tensor = sample_batch[0].to(device)\n",
    "        # input_tensor_aug = sample_batch[1].to(device)\n",
    "        feature1_warp, feature2_warp, feature1_orig, feature2_orig, mask1_orig, mask2_orig, mask1_warp, mask2_warp, \\\n",
    "            homography_grid, homography_grid_inv, warped_grid, warped_grid_inv, mesh_out, mesh_out_inv, im1_warp, im2_warp = model(input_tensor)\n",
    "        img1 = input_tensor[:, 0:3, :, :]\n",
    "        img2 = input_tensor[:, 3:6, :, :]\n",
    "        ones_mask = torch.ones_like(img1, dtype=torch.float, device=device)\n",
    "        img1_warp = spatial_transform_by_grid(img1, warped_grid, device=device)\n",
    "        img2_warp = spatial_transform_by_grid(img2, warped_grid_inv, device=device)\n",
    "        img1_mask_warp = spatial_transform_by_grid(ones_mask, warped_grid, device=device)\n",
    "        img2_mask_warp = spatial_transform_by_grid(ones_mask, warped_grid_inv, device=device)\n",
    "        img1_mask_warp[torch.where(img1_mask_warp < 0.4)] = 0\n",
    "        img2_mask_warp[torch.where(img2_mask_warp < 0.4)] = 0\n",
    "        img1_mask_warp = img1_mask_warp.bool()\n",
    "        img2_mask_warp = img2_mask_warp.bool()\n",
    "\n",
    "        img1 = tensor2img(img1)\n",
    "        img2 = tensor2img(img2)\n",
    "        img1_warp = tensor2img(img1_warp)\n",
    "        img2_warp = tensor2img(img2_warp)\n",
    "        img1 = img1.float()\n",
    "        img2 = img2.float()\n",
    "        img1_warp = img1_warp.float()\n",
    "        img2_warp = img2_warp.float()\n",
    "        ssim_tensor1 = ssim(img1 * img2_mask_warp, img2_warp * img2_mask_warp, data_range=255, size_average=False)\n",
    "        ssim_tensor2 = ssim(img2 * img1_mask_warp, img1_warp * img1_mask_warp, data_range=255, size_average=False)\n",
    "        psnr_tensor1 = calc_psnr(img1, img2_warp * img2_mask_warp)\n",
    "        psnr_tensor2 = calc_psnr(img2, img1_warp * img1_mask_warp)\n",
    "        ssim_result1.append(ssim_tensor1)\n",
    "        ssim_result2.append(ssim_tensor2)\n",
    "        psnr_result1.append(psnr_tensor1)\n",
    "        psnr_result2.append(psnr_tensor2)\n",
    "psnr_result1 = torch.cat(psnr_result1, dim=0)\n",
    "psnr_result2 = torch.cat(psnr_result2, dim=0)\n",
    "# avoid infinite psnr value result\n",
    "psnr_result1 = psnr_result1[~torch.isinf(psnr_result1)]\n",
    "psnr_result2 = psnr_result2[~torch.isinf(psnr_result2)]\n",
    "psnr_value1 = torch.mean(psnr_result1)\n",
    "psnr_value2 = torch.mean(psnr_result2)\n",
    "ssim_value1 = torch.mean(torch.cat(ssim_result1), dim=0)\n",
    "ssim_value2 = torch.mean(torch.cat(ssim_result2), dim=0)\n",
    "logger.info(\"Epoch {epoch} | psnr 1 {psnr_value1} | ssim 1 {ssim_value1} | psnr 2 {psnr_value2} | ssim 2 {ssim_value2} |\". \\\n",
    "    format(epoch=1, psnr_value1=psnr_value1, ssim_value1=ssim_value1, psnr_value2=psnr_value2, ssim_value2=ssim_value2))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wyq/anaconda3/envs/pytorch/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[2022-08-15 13:38:16,434][260973585.py][line:29][INFO] Parameters Setting:\n",
      "[2022-08-15 13:38:16,435][260973585.py][line:31][INFO] gpu: 1\n",
      "[2022-08-15 13:38:16,436][260973585.py][line:31][INFO] cpu: False\n",
      "[2022-08-15 13:38:16,437][260973585.py][line:31][INFO] seed: 1\n",
      "[2022-08-15 13:38:16,437][260973585.py][line:31][INFO] exp_name: test\n",
      "[2022-08-15 13:38:16,438][260973585.py][line:31][INFO] model_name: test\n",
      "[2022-08-15 13:38:16,438][260973585.py][line:31][INFO] resume: False\n",
      "[2022-08-15 13:38:16,439][260973585.py][line:31][INFO] num_workers: 8\n",
      "[2022-08-15 13:38:16,439][260973585.py][line:31][INFO] config: config.yaml\n",
      "[2022-08-15 13:38:16,440][260973585.py][line:31][INFO] model: DeepMeshFlow_s\n",
      "[2022-08-15 13:38:16,440][260973585.py][line:31][INFO] stage: train\n",
      "[2022-08-15 13:38:16,441][260973585.py][line:31][INFO] image_size: (128, 128)\n",
      "[2022-08-15 13:38:16,441][260973585.py][line:31][INFO] mesh_size_1: (2, 2)\n",
      "[2022-08-15 13:38:16,442][260973585.py][line:31][INFO] mesh_size_2: (5, 5)\n",
      "[2022-08-15 13:38:16,442][260973585.py][line:31][INFO] mesh_size_3: (17, 17)\n",
      "[2022-08-15 13:38:16,443][260973585.py][line:31][INFO] epoch: 100\n",
      "[2022-08-15 13:38:16,444][260973585.py][line:31][INFO] batch_size: 8\n",
      "[2022-08-15 13:38:16,444][260973585.py][line:31][INFO] lr: 0.0001\n",
      "[2022-08-15 13:38:16,445][260973585.py][line:31][INFO] weight_decay: 0.96\n",
      "[2022-08-15 13:38:16,445][260973585.py][line:31][INFO] print_every_iter: 50\n",
      "[2022-08-15 13:38:16,446][260973585.py][line:31][INFO] record_every_iter: 250\n",
      "[2022-08-15 13:38:16,446][260973585.py][line:31][INFO] save_every_epoch: 30\n",
      "[2022-08-15 13:38:16,447][260973585.py][line:31][INFO] share: 1\n",
      "[2022-08-15 13:38:16,447][260973585.py][line:31][INFO] train_data: /home/wyq/DeepImageStitching-pytorch/align_data/train/\n",
      "[2022-08-15 13:38:16,448][260973585.py][line:31][INFO] test_data: /home/wyq/DeepImageStitching-pytorch/align_data/test/\n",
      "[2022-08-15 13:38:16,448][260973585.py][line:31][INFO] data_inverse: 0\n",
      "[2022-08-15 13:38:16,449][260973585.py][line:31][INFO] data_random_inverse: 0\n",
      "[2022-08-15 13:38:16,449][260973585.py][line:31][INFO] brightness: 0.2\n",
      "[2022-08-15 13:38:16,450][260973585.py][line:31][INFO] contrast: 0.2\n",
      "[2022-08-15 13:38:16,450][260973585.py][line:31][INFO] saturation: 0.2\n",
      "[2022-08-15 13:38:16,451][260973585.py][line:31][INFO] loss_weight_lambda: 0.5\n",
      "[2022-08-15 13:38:16,451][260973585.py][line:31][INFO] loss_weight_mu: 0.01\n",
      "[2022-08-15 13:38:17,629][260973585.py][line:56][INFO] Training start.\n",
      "[2022-08-15 13:38:34,149][260973585.py][line:117][INFO] [1, 50] running_loss = 0.42193, loss1 = 0.21041, loss2 = 0.21152, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 13:38:47,957][260973585.py][line:117][INFO] [1, 100] running_loss = 0.43663, loss1 = 0.21760, loss2 = 0.21903, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 13:39:01,832][260973585.py][line:117][INFO] [1, 150] running_loss = 0.42661, loss1 = 0.21289, loss2 = 0.21372, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 13:39:15,784][260973585.py][line:117][INFO] [1, 200] running_loss = 0.41719, loss1 = 0.20807, loss2 = 0.20912, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 13:39:29,864][260973585.py][line:117][INFO] [1, 250] running_loss = 0.43640, loss1 = 0.21726, loss2 = 0.21915, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 13:39:43,934][260973585.py][line:117][INFO] [1, 300] running_loss = 0.40780, loss1 = 0.20288, loss2 = 0.20492, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 13:39:58,069][260973585.py][line:117][INFO] [1, 350] running_loss = 0.42553, loss1 = 0.21221, loss2 = 0.21332, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 13:40:12,211][260973585.py][line:117][INFO] [1, 400] running_loss = 0.41568, loss1 = 0.20748, loss2 = 0.20820, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 13:40:26,362][260973585.py][line:117][INFO] [1, 450] running_loss = 0.43344, loss1 = 0.21562, loss2 = 0.21781, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 13:40:40,464][260973585.py][line:117][INFO] [1, 500] running_loss = 0.42824, loss1 = 0.21347, loss2 = 0.21477, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 13:40:54,620][260973585.py][line:117][INFO] [1, 550] running_loss = 0.42136, loss1 = 0.20914, loss2 = 0.21222, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 13:41:08,701][260973585.py][line:117][INFO] [1, 600] running_loss = 0.41368, loss1 = 0.20609, loss2 = 0.20759, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 13:41:22,825][260973585.py][line:117][INFO] [1, 650] running_loss = 0.40934, loss1 = 0.20452, loss2 = 0.20483, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 13:41:36,960][260973585.py][line:117][INFO] [1, 700] running_loss = 0.41977, loss1 = 0.20946, loss2 = 0.21032, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 13:41:51,043][260973585.py][line:117][INFO] [1, 750] running_loss = 0.42719, loss1 = 0.21294, loss2 = 0.21425, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 13:42:05,175][260973585.py][line:117][INFO] [1, 800] running_loss = 0.40806, loss1 = 0.20383, loss2 = 0.20423, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 13:42:19,304][260973585.py][line:117][INFO] [1, 850] running_loss = 0.40570, loss1 = 0.20188, loss2 = 0.20382, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 13:42:33,426][260973585.py][line:117][INFO] [1, 900] running_loss = 0.42516, loss1 = 0.21125, loss2 = 0.21391, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 13:42:47,564][260973585.py][line:117][INFO] [1, 950] running_loss = 0.42694, loss1 = 0.21196, loss2 = 0.21499, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 13:43:01,669][260973585.py][line:117][INFO] [1, 1000] running_loss = 0.42430, loss1 = 0.21188, loss2 = 0.21242, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 13:43:15,808][260973585.py][line:117][INFO] [1, 1050] running_loss = 0.43995, loss1 = 0.21902, loss2 = 0.22094, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 13:43:29,920][260973585.py][line:117][INFO] [1, 1100] running_loss = 0.41972, loss1 = 0.20859, loss2 = 0.21112, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 13:43:44,067][260973585.py][line:117][INFO] [1, 1150] running_loss = 0.41920, loss1 = 0.20940, loss2 = 0.20979, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 13:43:58,210][260973585.py][line:117][INFO] [1, 1200] running_loss = 0.42552, loss1 = 0.21130, loss2 = 0.21422, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 13:44:12,337][260973585.py][line:117][INFO] [1, 1250] running_loss = 0.43035, loss1 = 0.21558, loss2 = 0.21477, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 13:44:26,469][260973585.py][line:117][INFO] [1, 1300] running_loss = 0.42400, loss1 = 0.21238, loss2 = 0.21162, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 13:44:40,599][260973585.py][line:117][INFO] [1, 1350] running_loss = 0.42599, loss1 = 0.21208, loss2 = 0.21392, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 13:44:54,757][260973585.py][line:117][INFO] [1, 1400] running_loss = 0.41309, loss1 = 0.20723, loss2 = 0.20586, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 13:45:08,880][260973585.py][line:117][INFO] [1, 1450] running_loss = 0.41665, loss1 = 0.20737, loss2 = 0.20927, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-08-15 13:45:23,016][260973585.py][line:117][INFO] [1, 1500] running_loss = 0.42783, loss1 = 0.21304, loss2 = 0.21480, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 13:45:37,144][260973585.py][line:117][INFO] [1, 1550] running_loss = 0.42482, loss1 = 0.21121, loss2 = 0.21361, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 13:45:51,305][260973585.py][line:117][INFO] [1, 1600] running_loss = 0.43282, loss1 = 0.21666, loss2 = 0.21616, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 13:46:05,420][260973585.py][line:117][INFO] [1, 1650] running_loss = 0.42224, loss1 = 0.21047, loss2 = 0.21178, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 13:46:19,596][260973585.py][line:117][INFO] [1, 1700] running_loss = 0.42362, loss1 = 0.21084, loss2 = 0.21278, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 13:46:33,763][260973585.py][line:117][INFO] [1, 1750] running_loss = 0.43405, loss1 = 0.21668, loss2 = 0.21737, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 13:46:47,903][260973585.py][line:117][INFO] [1, 1800] running_loss = 0.42172, loss1 = 0.21046, loss2 = 0.21125, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 13:47:02,080][260973585.py][line:117][INFO] [1, 1850] running_loss = 0.41715, loss1 = 0.20797, loss2 = 0.20919, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 13:47:16,195][260973585.py][line:117][INFO] [1, 1900] running_loss = 0.42765, loss1 = 0.21247, loss2 = 0.21518, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 13:47:30,357][260973585.py][line:117][INFO] [1, 1950] running_loss = 0.42829, loss1 = 0.21329, loss2 = 0.21500, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 13:47:44,484][260973585.py][line:117][INFO] [1, 2000] running_loss = 0.41482, loss1 = 0.20733, loss2 = 0.20749, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 13:47:58,653][260973585.py][line:117][INFO] [1, 2050] running_loss = 0.40790, loss1 = 0.20368, loss2 = 0.20422, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 13:48:12,812][260973585.py][line:117][INFO] [1, 2100] running_loss = 0.42116, loss1 = 0.20920, loss2 = 0.21196, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 13:48:26,966][260973585.py][line:117][INFO] [1, 2150] running_loss = 0.41734, loss1 = 0.20707, loss2 = 0.21026, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 13:48:41,097][260973585.py][line:117][INFO] [1, 2200] running_loss = 0.42201, loss1 = 0.20985, loss2 = 0.21216, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 13:48:55,217][260973585.py][line:117][INFO] [1, 2250] running_loss = 0.43598, loss1 = 0.21742, loss2 = 0.21856, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 13:49:09,314][260973585.py][line:117][INFO] [1, 2300] running_loss = 0.42079, loss1 = 0.21002, loss2 = 0.21078, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 13:49:23,465][260973585.py][line:117][INFO] [1, 2350] running_loss = 0.42704, loss1 = 0.21264, loss2 = 0.21440, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 13:49:37,576][260973585.py][line:117][INFO] [1, 2400] running_loss = 0.41489, loss1 = 0.20727, loss2 = 0.20762, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 13:49:51,701][260973585.py][line:117][INFO] [1, 2450] running_loss = 0.42021, loss1 = 0.20993, loss2 = 0.21028, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 13:50:05,811][260973585.py][line:117][INFO] [1, 2500] running_loss = 0.42008, loss1 = 0.20933, loss2 = 0.21075, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 13:50:19,925][260973585.py][line:117][INFO] [1, 2550] running_loss = 0.41364, loss1 = 0.20590, loss2 = 0.20774, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 13:50:34,065][260973585.py][line:117][INFO] [1, 2600] running_loss = 0.43258, loss1 = 0.21578, loss2 = 0.21680, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 13:50:48,181][260973585.py][line:117][INFO] [1, 2650] running_loss = 0.42476, loss1 = 0.21146, loss2 = 0.21330, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 13:51:02,293][260973585.py][line:117][INFO] [1, 2700] running_loss = 0.41971, loss1 = 0.20911, loss2 = 0.21060, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 13:51:16,443][260973585.py][line:117][INFO] [1, 2750] running_loss = 0.41087, loss1 = 0.20554, loss2 = 0.20533, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 13:51:30,596][260973585.py][line:117][INFO] [1, 2800] running_loss = 0.42217, loss1 = 0.20970, loss2 = 0.21246, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 13:51:44,747][260973585.py][line:117][INFO] [1, 2850] running_loss = 0.43012, loss1 = 0.21385, loss2 = 0.21628, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 13:51:58,894][260973585.py][line:117][INFO] [1, 2900] running_loss = 0.43106, loss1 = 0.21487, loss2 = 0.21619, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 13:52:13,082][260973585.py][line:117][INFO] [1, 2950] running_loss = 0.40508, loss1 = 0.20248, loss2 = 0.20261, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 13:52:27,213][260973585.py][line:117][INFO] [1, 3000] running_loss = 0.42550, loss1 = 0.21211, loss2 = 0.21340, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 13:52:41,369][260973585.py][line:117][INFO] [1, 3050] running_loss = 0.41845, loss1 = 0.20796, loss2 = 0.21049, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 13:52:55,569][260973585.py][line:117][INFO] [1, 3100] running_loss = 0.41821, loss1 = 0.20750, loss2 = 0.21071, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 13:53:09,712][260973585.py][line:117][INFO] [1, 3150] running_loss = 0.40715, loss1 = 0.20285, loss2 = 0.20430, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 13:53:23,854][260973585.py][line:117][INFO] [1, 3200] running_loss = 0.41522, loss1 = 0.20700, loss2 = 0.20822, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 13:53:37,964][260973585.py][line:117][INFO] [1, 3250] running_loss = 0.42209, loss1 = 0.21094, loss2 = 0.21115, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 13:53:52,078][260973585.py][line:117][INFO] [1, 3300] running_loss = 0.42192, loss1 = 0.20876, loss2 = 0.21315, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 13:54:06,215][260973585.py][line:117][INFO] [1, 3350] running_loss = 0.41313, loss1 = 0.20534, loss2 = 0.20779, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 13:54:20,388][260973585.py][line:117][INFO] [1, 3400] running_loss = 0.41670, loss1 = 0.20873, loss2 = 0.20796, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 13:54:34,488][260973585.py][line:117][INFO] [1, 3450] running_loss = 0.41097, loss1 = 0.20539, loss2 = 0.20558, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 13:54:48,629][260973585.py][line:117][INFO] [1, 3500] running_loss = 0.43258, loss1 = 0.21574, loss2 = 0.21683, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 13:55:02,748][260973585.py][line:117][INFO] [1, 3550] running_loss = 0.41750, loss1 = 0.20778, loss2 = 0.20972, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 13:55:16,943][260973585.py][line:117][INFO] [1, 3600] running_loss = 0.41716, loss1 = 0.20772, loss2 = 0.20943, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 13:55:31,118][260973585.py][line:117][INFO] [1, 3650] running_loss = 0.42232, loss1 = 0.20949, loss2 = 0.21283, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-08-15 13:55:45,310][260973585.py][line:117][INFO] [1, 3700] running_loss = 0.41366, loss1 = 0.20533, loss2 = 0.20833, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 13:55:59,452][260973585.py][line:117][INFO] [1, 3750] running_loss = 0.41888, loss1 = 0.20861, loss2 = 0.21027, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 13:56:13,569][260973585.py][line:117][INFO] [1, 3800] running_loss = 0.42233, loss1 = 0.21066, loss2 = 0.21167, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 13:56:27,720][260973585.py][line:117][INFO] [1, 3850] running_loss = 0.41073, loss1 = 0.20481, loss2 = 0.20592, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 13:56:41,900][260973585.py][line:117][INFO] [1, 3900] running_loss = 0.41999, loss1 = 0.20998, loss2 = 0.21001, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 13:56:56,033][260973585.py][line:117][INFO] [1, 3950] running_loss = 0.42267, loss1 = 0.21043, loss2 = 0.21224, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 13:57:10,168][260973585.py][line:117][INFO] [1, 4000] running_loss = 0.42466, loss1 = 0.21167, loss2 = 0.21299, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 13:57:24,356][260973585.py][line:117][INFO] [1, 4050] running_loss = 0.40227, loss1 = 0.20086, loss2 = 0.20141, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 13:57:38,521][260973585.py][line:117][INFO] [1, 4100] running_loss = 0.42416, loss1 = 0.21220, loss2 = 0.21196, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 13:57:52,617][260973585.py][line:117][INFO] [1, 4150] running_loss = 0.41842, loss1 = 0.20773, loss2 = 0.21069, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 13:58:06,797][260973585.py][line:117][INFO] [1, 4200] running_loss = 0.42104, loss1 = 0.21001, loss2 = 0.21103, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 13:58:20,947][260973585.py][line:117][INFO] [1, 4250] running_loss = 0.42902, loss1 = 0.21443, loss2 = 0.21459, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 13:58:35,045][260973585.py][line:117][INFO] [1, 4300] running_loss = 0.42504, loss1 = 0.21220, loss2 = 0.21283, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 13:58:49,179][260973585.py][line:117][INFO] [1, 4350] running_loss = 0.41982, loss1 = 0.20875, loss2 = 0.21106, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 13:59:03,337][260973585.py][line:117][INFO] [1, 4400] running_loss = 0.41831, loss1 = 0.20862, loss2 = 0.20969, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 13:59:17,484][260973585.py][line:117][INFO] [1, 4450] running_loss = 0.42803, loss1 = 0.21294, loss2 = 0.21509, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 13:59:31,631][260973585.py][line:117][INFO] [1, 4500] running_loss = 0.42000, loss1 = 0.20950, loss2 = 0.21050, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 13:59:45,752][260973585.py][line:117][INFO] [1, 4550] running_loss = 0.43991, loss1 = 0.21946, loss2 = 0.22045, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 13:59:59,901][260973585.py][line:117][INFO] [1, 4600] running_loss = 0.43365, loss1 = 0.21592, loss2 = 0.21773, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 14:00:14,028][260973585.py][line:117][INFO] [1, 4650] running_loss = 0.42528, loss1 = 0.21310, loss2 = 0.21219, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 14:00:28,161][260973585.py][line:117][INFO] [1, 4700] running_loss = 0.42658, loss1 = 0.21279, loss2 = 0.21379, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 14:00:42,316][260973585.py][line:117][INFO] [1, 4750] running_loss = 0.42792, loss1 = 0.21335, loss2 = 0.21457, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 14:00:56,485][260973585.py][line:117][INFO] [1, 4800] running_loss = 0.42854, loss1 = 0.21268, loss2 = 0.21587, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 14:01:10,628][260973585.py][line:117][INFO] [1, 4850] running_loss = 0.42689, loss1 = 0.21224, loss2 = 0.21465, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 14:01:24,777][260973585.py][line:117][INFO] [1, 4900] running_loss = 0.42626, loss1 = 0.21336, loss2 = 0.21289, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 14:01:38,944][260973585.py][line:117][INFO] [1, 4950] running_loss = 0.42750, loss1 = 0.21308, loss2 = 0.21442, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 14:01:53,105][260973585.py][line:117][INFO] [1, 5000] running_loss = 0.43114, loss1 = 0.21354, loss2 = 0.21760, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 14:02:07,264][260973585.py][line:117][INFO] [1, 5050] running_loss = 0.44238, loss1 = 0.22071, loss2 = 0.22167, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 14:02:21,437][260973585.py][line:117][INFO] [1, 5100] running_loss = 0.43801, loss1 = 0.21868, loss2 = 0.21933, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 14:02:35,608][260973585.py][line:117][INFO] [1, 5150] running_loss = 0.42705, loss1 = 0.21343, loss2 = 0.21361, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 14:02:49,758][260973585.py][line:117][INFO] [1, 5200] running_loss = 0.45052, loss1 = 0.22635, loss2 = 0.22417, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 14:03:03,929][260973585.py][line:117][INFO] [1, 5250] running_loss = 0.42970, loss1 = 0.21284, loss2 = 0.21686, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 14:03:18,112][260973585.py][line:117][INFO] [1, 5300] running_loss = 0.42740, loss1 = 0.21252, loss2 = 0.21488, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 14:03:32,258][260973585.py][line:117][INFO] [1, 5350] running_loss = 0.43751, loss1 = 0.21880, loss2 = 0.21871, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 14:03:46,396][260973585.py][line:117][INFO] [1, 5400] running_loss = 0.42980, loss1 = 0.21363, loss2 = 0.21618, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 14:04:00,552][260973585.py][line:117][INFO] [1, 5450] running_loss = 0.42058, loss1 = 0.21010, loss2 = 0.21048, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 14:04:14,685][260973585.py][line:117][INFO] [1, 5500] running_loss = 0.41848, loss1 = 0.20686, loss2 = 0.21162, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 14:04:28,855][260973585.py][line:117][INFO] [1, 5550] running_loss = 0.43599, loss1 = 0.21751, loss2 = 0.21848, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 14:04:42,994][260973585.py][line:117][INFO] [1, 5600] running_loss = 0.42461, loss1 = 0.21167, loss2 = 0.21294, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 14:04:57,202][260973585.py][line:117][INFO] [1, 5650] running_loss = 0.42907, loss1 = 0.21457, loss2 = 0.21450, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 14:05:11,368][260973585.py][line:117][INFO] [1, 5700] running_loss = 0.42421, loss1 = 0.21184, loss2 = 0.21237, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 14:05:25,487][260973585.py][line:117][INFO] [1, 5750] running_loss = 0.42576, loss1 = 0.21227, loss2 = 0.21349, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 14:05:39,611][260973585.py][line:117][INFO] [1, 5800] running_loss = 0.42139, loss1 = 0.20915, loss2 = 0.21224, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 14:05:53,751][260973585.py][line:117][INFO] [1, 5850] running_loss = 0.41765, loss1 = 0.20830, loss2 = 0.20935, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-08-15 14:06:07,892][260973585.py][line:117][INFO] [1, 5900] running_loss = 0.41944, loss1 = 0.21006, loss2 = 0.20938, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 14:06:22,032][260973585.py][line:117][INFO] [1, 5950] running_loss = 0.43492, loss1 = 0.21798, loss2 = 0.21694, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 14:06:36,148][260973585.py][line:117][INFO] [1, 6000] running_loss = 0.42731, loss1 = 0.21293, loss2 = 0.21438, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 14:06:50,298][260973585.py][line:117][INFO] [1, 6050] running_loss = 0.43035, loss1 = 0.21423, loss2 = 0.21612, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 14:07:04,425][260973585.py][line:117][INFO] [1, 6100] running_loss = 0.41667, loss1 = 0.20709, loss2 = 0.20958, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 14:07:18,585][260973585.py][line:117][INFO] [1, 6150] running_loss = 0.43348, loss1 = 0.21637, loss2 = 0.21711, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 14:07:32,789][260973585.py][line:117][INFO] [1, 6200] running_loss = 0.42039, loss1 = 0.20882, loss2 = 0.21157, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 14:07:46,935][260973585.py][line:117][INFO] [1, 6250] running_loss = 0.41621, loss1 = 0.20616, loss2 = 0.21005, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0001000\n",
      "[2022-08-15 14:08:02,000][260973585.py][line:117][INFO] [2, 50] running_loss = 0.41105, loss1 = 0.20463, loss2 = 0.20641, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:08:16,114][260973585.py][line:117][INFO] [2, 100] running_loss = 0.41353, loss1 = 0.20454, loss2 = 0.20899, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:08:30,284][260973585.py][line:117][INFO] [2, 150] running_loss = 0.43624, loss1 = 0.21859, loss2 = 0.21765, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:08:44,377][260973585.py][line:117][INFO] [2, 200] running_loss = 0.42189, loss1 = 0.21068, loss2 = 0.21122, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:08:58,500][260973585.py][line:117][INFO] [2, 250] running_loss = 0.42315, loss1 = 0.21118, loss2 = 0.21197, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:09:12,691][260973585.py][line:117][INFO] [2, 300] running_loss = 0.43452, loss1 = 0.21676, loss2 = 0.21776, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:09:26,801][260973585.py][line:117][INFO] [2, 350] running_loss = 0.44660, loss1 = 0.22159, loss2 = 0.22501, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:09:40,956][260973585.py][line:117][INFO] [2, 400] running_loss = 0.40940, loss1 = 0.20310, loss2 = 0.20630, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:09:55,089][260973585.py][line:117][INFO] [2, 450] running_loss = 0.43596, loss1 = 0.21726, loss2 = 0.21870, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:10:09,224][260973585.py][line:117][INFO] [2, 500] running_loss = 0.45594, loss1 = 0.22940, loss2 = 0.22654, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:10:23,364][260973585.py][line:117][INFO] [2, 550] running_loss = 0.43870, loss1 = 0.21861, loss2 = 0.22010, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:10:37,494][260973585.py][line:117][INFO] [2, 600] running_loss = 0.42819, loss1 = 0.21392, loss2 = 0.21428, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:10:51,604][260973585.py][line:117][INFO] [2, 650] running_loss = 0.43026, loss1 = 0.21620, loss2 = 0.21406, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:11:05,701][260973585.py][line:117][INFO] [2, 700] running_loss = 0.43487, loss1 = 0.21686, loss2 = 0.21800, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:11:19,816][260973585.py][line:117][INFO] [2, 750] running_loss = 0.42398, loss1 = 0.21217, loss2 = 0.21181, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:11:33,948][260973585.py][line:117][INFO] [2, 800] running_loss = 0.43370, loss1 = 0.21482, loss2 = 0.21888, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:11:48,095][260973585.py][line:117][INFO] [2, 850] running_loss = 0.43127, loss1 = 0.21447, loss2 = 0.21680, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:12:02,192][260973585.py][line:117][INFO] [2, 900] running_loss = 0.44360, loss1 = 0.22300, loss2 = 0.22061, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:12:16,285][260973585.py][line:117][INFO] [2, 950] running_loss = 0.43215, loss1 = 0.21509, loss2 = 0.21706, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:12:30,436][260973585.py][line:117][INFO] [2, 1000] running_loss = 0.42077, loss1 = 0.20899, loss2 = 0.21178, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:12:44,584][260973585.py][line:117][INFO] [2, 1050] running_loss = 0.42751, loss1 = 0.21297, loss2 = 0.21454, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:12:58,660][260973585.py][line:117][INFO] [2, 1100] running_loss = 0.43342, loss1 = 0.21716, loss2 = 0.21626, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:13:12,822][260973585.py][line:117][INFO] [2, 1150] running_loss = 0.41852, loss1 = 0.20871, loss2 = 0.20982, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:13:26,936][260973585.py][line:117][INFO] [2, 1200] running_loss = 0.41238, loss1 = 0.20470, loss2 = 0.20769, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:13:41,045][260973585.py][line:117][INFO] [2, 1250] running_loss = 0.43515, loss1 = 0.21744, loss2 = 0.21770, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:13:55,168][260973585.py][line:117][INFO] [2, 1300] running_loss = 0.43146, loss1 = 0.21529, loss2 = 0.21618, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:14:09,261][260973585.py][line:117][INFO] [2, 1350] running_loss = 0.43252, loss1 = 0.21569, loss2 = 0.21683, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:14:23,404][260973585.py][line:117][INFO] [2, 1400] running_loss = 0.41510, loss1 = 0.20852, loss2 = 0.20658, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:14:37,557][260973585.py][line:117][INFO] [2, 1450] running_loss = 0.41994, loss1 = 0.20931, loss2 = 0.21062, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:14:51,684][260973585.py][line:117][INFO] [2, 1500] running_loss = 0.44993, loss1 = 0.22579, loss2 = 0.22415, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:15:05,870][260973585.py][line:117][INFO] [2, 1550] running_loss = 0.43843, loss1 = 0.21822, loss2 = 0.22020, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:15:20,027][260973585.py][line:117][INFO] [2, 1600] running_loss = 0.43459, loss1 = 0.21591, loss2 = 0.21868, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:15:34,136][260973585.py][line:117][INFO] [2, 1650] running_loss = 0.41824, loss1 = 0.20883, loss2 = 0.20941, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:15:48,326][260973585.py][line:117][INFO] [2, 1700] running_loss = 0.42129, loss1 = 0.20896, loss2 = 0.21233, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:16:02,460][260973585.py][line:117][INFO] [2, 1750] running_loss = 0.43042, loss1 = 0.21408, loss2 = 0.21634, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:16:16,604][260973585.py][line:117][INFO] [2, 1800] running_loss = 0.41600, loss1 = 0.20740, loss2 = 0.20860, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-08-15 14:16:30,761][260973585.py][line:117][INFO] [2, 1850] running_loss = 0.43754, loss1 = 0.21757, loss2 = 0.21997, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:16:44,865][260973585.py][line:117][INFO] [2, 1900] running_loss = 0.43436, loss1 = 0.21547, loss2 = 0.21889, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:16:59,021][260973585.py][line:117][INFO] [2, 1950] running_loss = 0.42147, loss1 = 0.20756, loss2 = 0.21391, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:17:13,124][260973585.py][line:117][INFO] [2, 2000] running_loss = 0.42123, loss1 = 0.21151, loss2 = 0.20972, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:17:27,233][260973585.py][line:117][INFO] [2, 2050] running_loss = 0.42680, loss1 = 0.21080, loss2 = 0.21599, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:17:41,325][260973585.py][line:117][INFO] [2, 2100] running_loss = 0.43125, loss1 = 0.21238, loss2 = 0.21887, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:17:55,491][260973585.py][line:117][INFO] [2, 2150] running_loss = 0.42896, loss1 = 0.21320, loss2 = 0.21576, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:18:09,637][260973585.py][line:117][INFO] [2, 2200] running_loss = 0.42648, loss1 = 0.21188, loss2 = 0.21460, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:18:23,725][260973585.py][line:117][INFO] [2, 2250] running_loss = 0.42534, loss1 = 0.21047, loss2 = 0.21487, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:18:37,883][260973585.py][line:117][INFO] [2, 2300] running_loss = 0.43642, loss1 = 0.21576, loss2 = 0.22066, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:18:52,004][260973585.py][line:117][INFO] [2, 2350] running_loss = 0.44413, loss1 = 0.21987, loss2 = 0.22427, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:19:06,154][260973585.py][line:117][INFO] [2, 2400] running_loss = 0.41690, loss1 = 0.20751, loss2 = 0.20938, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:19:20,293][260973585.py][line:117][INFO] [2, 2450] running_loss = 0.42537, loss1 = 0.21091, loss2 = 0.21446, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:19:34,418][260973585.py][line:117][INFO] [2, 2500] running_loss = 0.44626, loss1 = 0.22293, loss2 = 0.22333, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:19:48,532][260973585.py][line:117][INFO] [2, 2550] running_loss = 0.43915, loss1 = 0.21977, loss2 = 0.21938, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:20:02,628][260973585.py][line:117][INFO] [2, 2600] running_loss = 0.44793, loss1 = 0.22167, loss2 = 0.22626, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:20:16,744][260973585.py][line:117][INFO] [2, 2650] running_loss = 0.42892, loss1 = 0.21397, loss2 = 0.21495, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:20:30,881][260973585.py][line:117][INFO] [2, 2700] running_loss = 0.43556, loss1 = 0.21631, loss2 = 0.21925, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:20:45,011][260973585.py][line:117][INFO] [2, 2750] running_loss = 0.43872, loss1 = 0.21961, loss2 = 0.21911, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:20:59,152][260973585.py][line:117][INFO] [2, 2800] running_loss = 0.42235, loss1 = 0.21015, loss2 = 0.21220, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:21:13,290][260973585.py][line:117][INFO] [2, 2850] running_loss = 0.42992, loss1 = 0.21559, loss2 = 0.21433, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:21:27,427][260973585.py][line:117][INFO] [2, 2900] running_loss = 0.44638, loss1 = 0.22351, loss2 = 0.22286, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:21:41,517][260973585.py][line:117][INFO] [2, 2950] running_loss = 0.44429, loss1 = 0.22209, loss2 = 0.22220, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:21:55,624][260973585.py][line:117][INFO] [2, 3000] running_loss = 0.41740, loss1 = 0.20623, loss2 = 0.21117, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:22:09,764][260973585.py][line:117][INFO] [2, 3050] running_loss = 0.40499, loss1 = 0.20195, loss2 = 0.20304, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:22:23,860][260973585.py][line:117][INFO] [2, 3100] running_loss = 0.45976, loss1 = 0.22929, loss2 = 0.23047, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:22:37,967][260973585.py][line:117][INFO] [2, 3150] running_loss = 0.43903, loss1 = 0.21872, loss2 = 0.22032, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:22:52,112][260973585.py][line:117][INFO] [2, 3200] running_loss = 0.42279, loss1 = 0.21078, loss2 = 0.21201, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:23:06,192][260973585.py][line:117][INFO] [2, 3250] running_loss = 0.45663, loss1 = 0.22754, loss2 = 0.22908, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:23:20,308][260973585.py][line:117][INFO] [2, 3300] running_loss = 0.44306, loss1 = 0.22062, loss2 = 0.22243, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:23:34,499][260973585.py][line:117][INFO] [2, 3350] running_loss = 0.44763, loss1 = 0.22242, loss2 = 0.22521, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:23:48,639][260973585.py][line:117][INFO] [2, 3400] running_loss = 0.44647, loss1 = 0.22311, loss2 = 0.22336, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:24:02,796][260973585.py][line:117][INFO] [2, 3450] running_loss = 0.44754, loss1 = 0.22296, loss2 = 0.22458, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:24:16,917][260973585.py][line:117][INFO] [2, 3500] running_loss = 0.44064, loss1 = 0.21917, loss2 = 0.22147, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:24:31,024][260973585.py][line:117][INFO] [2, 3550] running_loss = 0.44238, loss1 = 0.21994, loss2 = 0.22244, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:24:45,179][260973585.py][line:117][INFO] [2, 3600] running_loss = 0.44001, loss1 = 0.21914, loss2 = 0.22087, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:24:59,308][260973585.py][line:117][INFO] [2, 3650] running_loss = 0.43188, loss1 = 0.21543, loss2 = 0.21646, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:25:13,459][260973585.py][line:117][INFO] [2, 3700] running_loss = 0.44391, loss1 = 0.22246, loss2 = 0.22145, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:25:27,603][260973585.py][line:117][INFO] [2, 3750] running_loss = 0.44940, loss1 = 0.22325, loss2 = 0.22616, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:25:41,753][260973585.py][line:117][INFO] [2, 3800] running_loss = 0.43235, loss1 = 0.21601, loss2 = 0.21634, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:25:55,835][260973585.py][line:117][INFO] [2, 3850] running_loss = 0.41987, loss1 = 0.20843, loss2 = 0.21144, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:26:09,925][260973585.py][line:117][INFO] [2, 3900] running_loss = 0.42223, loss1 = 0.21056, loss2 = 0.21167, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:26:24,080][260973585.py][line:117][INFO] [2, 3950] running_loss = 0.42535, loss1 = 0.21148, loss2 = 0.21387, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:26:38,177][260973585.py][line:117][INFO] [2, 4000] running_loss = 0.43771, loss1 = 0.21846, loss2 = 0.21925, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-08-15 14:26:52,320][260973585.py][line:117][INFO] [2, 4050] running_loss = 0.43460, loss1 = 0.21640, loss2 = 0.21820, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:27:06,445][260973585.py][line:117][INFO] [2, 4100] running_loss = 0.42888, loss1 = 0.21356, loss2 = 0.21532, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:27:20,568][260973585.py][line:117][INFO] [2, 4150] running_loss = 0.43595, loss1 = 0.21755, loss2 = 0.21840, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:27:34,684][260973585.py][line:117][INFO] [2, 4200] running_loss = 0.42673, loss1 = 0.21266, loss2 = 0.21407, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:27:48,807][260973585.py][line:117][INFO] [2, 4250] running_loss = 0.44147, loss1 = 0.22005, loss2 = 0.22142, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:28:02,936][260973585.py][line:117][INFO] [2, 4300] running_loss = 0.44176, loss1 = 0.22025, loss2 = 0.22151, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:28:17,071][260973585.py][line:117][INFO] [2, 4350] running_loss = 0.44720, loss1 = 0.22160, loss2 = 0.22559, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:28:31,176][260973585.py][line:117][INFO] [2, 4400] running_loss = 0.42681, loss1 = 0.21274, loss2 = 0.21407, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:28:45,277][260973585.py][line:117][INFO] [2, 4450] running_loss = 0.43226, loss1 = 0.21595, loss2 = 0.21631, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:28:59,364][260973585.py][line:117][INFO] [2, 4500] running_loss = 0.42985, loss1 = 0.21368, loss2 = 0.21618, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:29:13,472][260973585.py][line:117][INFO] [2, 4550] running_loss = 0.42667, loss1 = 0.21218, loss2 = 0.21449, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:29:27,610][260973585.py][line:117][INFO] [2, 4600] running_loss = 0.42883, loss1 = 0.21448, loss2 = 0.21434, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:29:41,719][260973585.py][line:117][INFO] [2, 4650] running_loss = 0.44122, loss1 = 0.22016, loss2 = 0.22105, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:29:55,840][260973585.py][line:117][INFO] [2, 4700] running_loss = 0.44360, loss1 = 0.22156, loss2 = 0.22204, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:30:09,993][260973585.py][line:117][INFO] [2, 4750] running_loss = 0.42446, loss1 = 0.21103, loss2 = 0.21343, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:30:24,108][260973585.py][line:117][INFO] [2, 4800] running_loss = 0.44212, loss1 = 0.22081, loss2 = 0.22131, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:30:38,260][260973585.py][line:117][INFO] [2, 4850] running_loss = 0.43110, loss1 = 0.21520, loss2 = 0.21590, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:30:52,359][260973585.py][line:117][INFO] [2, 4900] running_loss = 0.42543, loss1 = 0.21140, loss2 = 0.21403, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:31:06,498][260973585.py][line:117][INFO] [2, 4950] running_loss = 0.43689, loss1 = 0.21830, loss2 = 0.21859, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:31:20,612][260973585.py][line:117][INFO] [2, 5000] running_loss = 0.43545, loss1 = 0.21592, loss2 = 0.21953, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:31:34,769][260973585.py][line:117][INFO] [2, 5050] running_loss = 0.42350, loss1 = 0.21082, loss2 = 0.21268, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:31:48,860][260973585.py][line:117][INFO] [2, 5100] running_loss = 0.41444, loss1 = 0.20733, loss2 = 0.20711, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:32:02,985][260973585.py][line:117][INFO] [2, 5150] running_loss = 0.44480, loss1 = 0.22264, loss2 = 0.22217, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:32:17,095][260973585.py][line:117][INFO] [2, 5200] running_loss = 0.42928, loss1 = 0.21382, loss2 = 0.21547, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:32:31,231][260973585.py][line:117][INFO] [2, 5250] running_loss = 0.42061, loss1 = 0.20920, loss2 = 0.21142, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:32:45,361][260973585.py][line:117][INFO] [2, 5300] running_loss = 0.42687, loss1 = 0.21135, loss2 = 0.21552, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:32:59,484][260973585.py][line:117][INFO] [2, 5350] running_loss = 0.44558, loss1 = 0.22242, loss2 = 0.22316, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:33:13,601][260973585.py][line:117][INFO] [2, 5400] running_loss = 0.45076, loss1 = 0.22441, loss2 = 0.22635, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:33:27,780][260973585.py][line:117][INFO] [2, 5450] running_loss = 0.47338, loss1 = 0.23585, loss2 = 0.23753, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:33:41,881][260973585.py][line:117][INFO] [2, 5500] running_loss = 0.44761, loss1 = 0.22280, loss2 = 0.22480, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:33:55,997][260973585.py][line:117][INFO] [2, 5550] running_loss = 0.46958, loss1 = 0.23433, loss2 = 0.23525, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:34:10,134][260973585.py][line:117][INFO] [2, 5600] running_loss = 0.46476, loss1 = 0.23161, loss2 = 0.23315, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:34:24,275][260973585.py][line:117][INFO] [2, 5650] running_loss = 0.46277, loss1 = 0.23065, loss2 = 0.23212, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:34:38,399][260973585.py][line:117][INFO] [2, 5700] running_loss = 0.46549, loss1 = 0.23187, loss2 = 0.23361, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:34:52,542][260973585.py][line:117][INFO] [2, 5750] running_loss = 0.48040, loss1 = 0.24034, loss2 = 0.24006, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:35:06,655][260973585.py][line:117][INFO] [2, 5800] running_loss = 0.46469, loss1 = 0.23128, loss2 = 0.23342, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:35:20,829][260973585.py][line:117][INFO] [2, 5850] running_loss = 0.44939, loss1 = 0.22416, loss2 = 0.22523, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:35:34,972][260973585.py][line:117][INFO] [2, 5900] running_loss = 0.46953, loss1 = 0.23379, loss2 = 0.23574, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:35:49,088][260973585.py][line:117][INFO] [2, 5950] running_loss = 0.44036, loss1 = 0.21943, loss2 = 0.22093, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:36:03,241][260973585.py][line:117][INFO] [2, 6000] running_loss = 0.44870, loss1 = 0.22337, loss2 = 0.22533, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:36:17,384][260973585.py][line:117][INFO] [2, 6050] running_loss = 0.44686, loss1 = 0.22314, loss2 = 0.22371, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:36:31,524][260973585.py][line:117][INFO] [2, 6100] running_loss = 0.45326, loss1 = 0.22643, loss2 = 0.22683, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:36:45,639][260973585.py][line:117][INFO] [2, 6150] running_loss = 0.45086, loss1 = 0.22434, loss2 = 0.22652, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:36:59,820][260973585.py][line:117][INFO] [2, 6200] running_loss = 0.45021, loss1 = 0.22511, loss2 = 0.22510, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-08-15 14:37:13,924][260973585.py][line:117][INFO] [2, 6250] running_loss = 0.44215, loss1 = 0.22070, loss2 = 0.22144, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000960\n",
      "[2022-08-15 14:37:29,040][260973585.py][line:117][INFO] [3, 50] running_loss = 0.44588, loss1 = 0.22303, loss2 = 0.22285, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000922\n",
      "[2022-08-15 14:37:43,180][260973585.py][line:117][INFO] [3, 100] running_loss = 0.44888, loss1 = 0.22407, loss2 = 0.22481, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000922\n",
      "[2022-08-15 14:37:57,281][260973585.py][line:117][INFO] [3, 150] running_loss = 0.43093, loss1 = 0.21544, loss2 = 0.21549, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000922\n",
      "[2022-08-15 14:38:11,397][260973585.py][line:117][INFO] [3, 200] running_loss = 0.43364, loss1 = 0.21734, loss2 = 0.21630, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000922\n",
      "[2022-08-15 14:38:25,516][260973585.py][line:117][INFO] [3, 250] running_loss = 0.44030, loss1 = 0.22020, loss2 = 0.22009, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000922\n",
      "[2022-08-15 14:38:39,663][260973585.py][line:117][INFO] [3, 300] running_loss = 0.45115, loss1 = 0.22478, loss2 = 0.22638, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000922\n",
      "[2022-08-15 14:38:53,781][260973585.py][line:117][INFO] [3, 350] running_loss = 0.45157, loss1 = 0.22540, loss2 = 0.22616, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000922\n",
      "[2022-08-15 14:39:07,895][260973585.py][line:117][INFO] [3, 400] running_loss = 0.45306, loss1 = 0.22561, loss2 = 0.22746, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000922\n",
      "[2022-08-15 14:39:22,005][260973585.py][line:117][INFO] [3, 450] running_loss = 0.44099, loss1 = 0.22083, loss2 = 0.22016, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000922\n",
      "[2022-08-15 14:39:36,118][260973585.py][line:117][INFO] [3, 500] running_loss = 0.44787, loss1 = 0.22354, loss2 = 0.22434, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000922\n",
      "[2022-08-15 14:39:50,247][260973585.py][line:117][INFO] [3, 550] running_loss = 0.44221, loss1 = 0.22097, loss2 = 0.22124, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000922\n",
      "[2022-08-15 14:40:04,345][260973585.py][line:117][INFO] [3, 600] running_loss = 0.44678, loss1 = 0.22336, loss2 = 0.22342, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000922\n",
      "[2022-08-15 14:40:18,499][260973585.py][line:117][INFO] [3, 650] running_loss = 0.45410, loss1 = 0.22606, loss2 = 0.22804, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000922\n",
      "[2022-08-15 14:40:32,614][260973585.py][line:117][INFO] [3, 700] running_loss = 0.44818, loss1 = 0.22240, loss2 = 0.22577, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000922\n",
      "[2022-08-15 14:40:46,721][260973585.py][line:117][INFO] [3, 750] running_loss = 0.46128, loss1 = 0.22992, loss2 = 0.23136, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000922\n",
      "[2022-08-15 14:41:00,861][260973585.py][line:117][INFO] [3, 800] running_loss = 0.44756, loss1 = 0.22480, loss2 = 0.22277, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000922\n",
      "[2022-08-15 14:41:15,037][260973585.py][line:117][INFO] [3, 850] running_loss = 0.44098, loss1 = 0.21932, loss2 = 0.22166, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000922\n",
      "[2022-08-15 14:41:29,105][260973585.py][line:117][INFO] [3, 900] running_loss = 0.44262, loss1 = 0.21955, loss2 = 0.22307, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000922\n",
      "[2022-08-15 14:41:43,248][260973585.py][line:117][INFO] [3, 950] running_loss = 0.43474, loss1 = 0.21748, loss2 = 0.21727, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000922\n",
      "[2022-08-15 14:41:57,387][260973585.py][line:117][INFO] [3, 1000] running_loss = 0.43822, loss1 = 0.21809, loss2 = 0.22014, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000922\n",
      "[2022-08-15 14:42:11,534][260973585.py][line:117][INFO] [3, 1050] running_loss = 0.44467, loss1 = 0.22168, loss2 = 0.22299, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000922\n",
      "[2022-08-15 14:42:25,641][260973585.py][line:117][INFO] [3, 1100] running_loss = 0.45051, loss1 = 0.22634, loss2 = 0.22417, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000922\n",
      "[2022-08-15 14:42:39,776][260973585.py][line:117][INFO] [3, 1150] running_loss = 0.43785, loss1 = 0.21803, loss2 = 0.21982, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000922\n",
      "[2022-08-15 14:42:53,882][260973585.py][line:117][INFO] [3, 1200] running_loss = 0.44618, loss1 = 0.22230, loss2 = 0.22388, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000922\n",
      "[2022-08-15 14:43:07,977][260973585.py][line:117][INFO] [3, 1250] running_loss = 0.44482, loss1 = 0.22194, loss2 = 0.22288, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000922\n",
      "[2022-08-15 14:43:22,158][260973585.py][line:117][INFO] [3, 1300] running_loss = 0.43265, loss1 = 0.21633, loss2 = 0.21631, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000922\n",
      "[2022-08-15 14:43:36,280][260973585.py][line:117][INFO] [3, 1350] running_loss = 0.44339, loss1 = 0.22164, loss2 = 0.22175, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000922\n",
      "[2022-08-15 14:43:50,432][260973585.py][line:117][INFO] [3, 1400] running_loss = 0.43938, loss1 = 0.21877, loss2 = 0.22061, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000922\n",
      "[2022-08-15 14:44:04,526][260973585.py][line:117][INFO] [3, 1450] running_loss = 0.43970, loss1 = 0.21967, loss2 = 0.22003, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000922\n",
      "[2022-08-15 14:44:18,696][260973585.py][line:117][INFO] [3, 1500] running_loss = 0.43925, loss1 = 0.21978, loss2 = 0.21948, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000922\n",
      "[2022-08-15 14:44:32,815][260973585.py][line:117][INFO] [3, 1550] running_loss = 0.43353, loss1 = 0.21639, loss2 = 0.21713, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000922\n",
      "[2022-08-15 14:44:46,961][260973585.py][line:117][INFO] [3, 1600] running_loss = 0.43743, loss1 = 0.21907, loss2 = 0.21836, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000922\n",
      "[2022-08-15 14:45:01,115][260973585.py][line:117][INFO] [3, 1650] running_loss = 0.44212, loss1 = 0.22075, loss2 = 0.22137, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000922\n",
      "[2022-08-15 14:45:15,238][260973585.py][line:117][INFO] [3, 1700] running_loss = 0.42369, loss1 = 0.21097, loss2 = 0.21272, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000922\n",
      "[2022-08-15 14:45:29,356][260973585.py][line:117][INFO] [3, 1750] running_loss = 0.43734, loss1 = 0.21833, loss2 = 0.21901, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000922\n",
      "[2022-08-15 14:45:43,452][260973585.py][line:117][INFO] [3, 1800] running_loss = 0.43896, loss1 = 0.21949, loss2 = 0.21947, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000922\n",
      "[2022-08-15 14:45:57,564][260973585.py][line:117][INFO] [3, 1850] running_loss = 0.44820, loss1 = 0.22261, loss2 = 0.22558, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000922\n",
      "[2022-08-15 14:46:11,680][260973585.py][line:117][INFO] [3, 1900] running_loss = 0.44826, loss1 = 0.22445, loss2 = 0.22381, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000922\n",
      "[2022-08-15 14:46:25,820][260973585.py][line:117][INFO] [3, 1950] running_loss = 0.43000, loss1 = 0.21373, loss2 = 0.21627, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000922\n",
      "[2022-08-15 14:46:39,916][260973585.py][line:117][INFO] [3, 2000] running_loss = 0.44255, loss1 = 0.22034, loss2 = 0.22221, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000922\n",
      "[2022-08-15 14:46:54,049][260973585.py][line:117][INFO] [3, 2050] running_loss = 0.43625, loss1 = 0.21728, loss2 = 0.21897, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000922\n",
      "[2022-08-15 14:47:08,212][260973585.py][line:117][INFO] [3, 2100] running_loss = 0.43525, loss1 = 0.21572, loss2 = 0.21954, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000922\n",
      "[2022-08-15 14:47:22,293][260973585.py][line:117][INFO] [3, 2150] running_loss = 0.43622, loss1 = 0.21720, loss2 = 0.21901, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-08-15 14:47:36,456][260973585.py][line:117][INFO] [3, 2200] running_loss = 0.44416, loss1 = 0.22108, loss2 = 0.22308, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000922\n",
      "[2022-08-15 14:47:50,605][260973585.py][line:117][INFO] [3, 2250] running_loss = 0.43679, loss1 = 0.21683, loss2 = 0.21996, L_ab = 0.00000, L_inversable = 0.00000, learning_rate = 0.0000922\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 63>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    101\u001b[0m     break_flag \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    102\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 103\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    105\u001b[0m running_loss[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from dataset import align_dataset\n",
    "\n",
    "from config.option import args\n",
    "from utils.get_logger import Get_logger\n",
    "from utils.metrics import calc_psnr\n",
    "from utils.solve_DLT import solve_mesh_flow_DLT, spatial_transform_by_grid\n",
    "from utils.toolkit import tensor2img\n",
    "from pytorch_msssim.ssim import ssim\n",
    "from models.deep_mesh_flow import DeepMeshFlow\n",
    "from models.deep_mesh_flow_s import DeepMeshFlow_s\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "if args.cpu:\n",
    "    device = torch.device(\"cpu\")\n",
    "else:\n",
    "    # os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu\n",
    "    device = torch.device(\"cuda:{gpu_num}\".format(gpu_num=args.gpu))    \n",
    "\n",
    "# initialize logging file\n",
    "logger = Get_logger(os.path.join(\"log\", args.exp_name + \".log\"))\n",
    "logger.info(\"Parameters Setting:\")\n",
    "for i in args.__dict__:\n",
    "    logger.info(\"{param_name}: {param_value}\".format(param_name=i, param_value=args.__dict__[i]))\n",
    "model_list = {\"DeepMeshFlow\": DeepMeshFlow, \"DeepMeshFlow_s\": DeepMeshFlow_s}\n",
    "model = model_list[args.model](args=args, device=device)\n",
    "model = model.to(device)\n",
    "train_dataset = align_dataset(args=args)\n",
    "val_dataset = align_dataset(args=args, validation=True)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.num_workers)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer=optimizer, step_size=1, gamma=args.weight_decay)\n",
    "\n",
    "# result save\n",
    "# writer = SummaryWriter(os.path.join(\"runs\", args.exp_name))\n",
    "model_save_path = os.path.join(\"checkpoints\", args.exp_name)\n",
    "if not os.path.isdir(model_save_path):\n",
    "    os.mkdir(model_save_path)\n",
    "\n",
    "print_every_iter = args.print_every_iter\n",
    "record_every_iter = args.record_every_iter\n",
    "save_every_epoch = args.save_every_epoch\n",
    "\n",
    "epochs = args.epoch\n",
    "start_epoch = 0\n",
    "tb_index = 0 + ((len(train_dataset) // args.batch_size) // args.record_every_iter) * start_epoch\n",
    "# patch_size = (args.image_size[0] // (args.mesh_size_3[0] - 1), args.image_size[1] // (args.mesh_size_3[1] - 1))\n",
    "logger.info(\"Training start.\")\n",
    "if args.resume:\n",
    "    loaded = torch.load(os.path.join(\"checkpoints\", args.exp_name, args.model_name + \"_latest.pth\"))\n",
    "    model.load_state_dict(loaded['state_dict'])\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=loaded['lr'])\n",
    "    start_epoch = loaded['epoch']\n",
    "break_flag = 0\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    model.train()\n",
    "    running_loss = [0., 0., 0., 0., 0.]\n",
    "    print_loss = 0.\n",
    "    print_loss_items = [0., 0., 0., 0.,]\n",
    "    for i, sample_batch in enumerate(train_loader):\n",
    "        input_tensor = sample_batch[0].to(device)\n",
    "        img_names = sample_batch[1]\n",
    "        # loss for DeepMeshFlow\n",
    "        if args.model == \"DeepMeshFlow\":\n",
    "            feature1_warp, feature2_warp, feature1_orig, feature2_orig, mask1_orig, mask2_orig, mask1_warp, mask2_warp, \\\n",
    "            homography_grid, homography_grid_inv, warped_grid, warped_grid_inv, mesh_out, mesh_out_inv, im1_warp, im2_warp = model(input_tensor)\n",
    "\n",
    "            ln = torch.sum(mask1_warp * mask2_orig * torch.abs(feature2_orig - feature1_warp)) / torch.sum(mask1_warp * mask2_orig)\n",
    "\n",
    "            ln_inv = torch.sum(mask1_orig * mask2_warp * torch.abs(feature1_orig - feature2_warp)) / torch.sum(mask1_orig * mask2_warp)\n",
    "            # TODO: Does L ab loss should be normalized ?\n",
    "            L_ab = -1 * torch.abs(feature1_orig - feature2_orig).mean()\n",
    "            # Identity = torch.eye(3, dtype=torch.float, device=device).unsqueeze(0).expand(homography_grid.shape[0], -1, -1)\n",
    "            # L_inverse = torch.mean(torch.abs(torch.matmul(homography_grid, homography_grid_inv) - Identity))\n",
    "            # loss = ln + ln_inv + args.loss_weight_lambda * L_ab + args.loss_weight_mu * L_inverse\n",
    "            loss = ln + ln_inv + args.loss_weight_lambda * L_ab\n",
    "#             if torch.isnan(ln):\n",
    "#                 print(\"ln is nan.\")\n",
    "#             if torch.isnan(ln_inv):\n",
    "#                 print(\"ln_inv is nan.\")\n",
    "#             if torch.isnan(L_ab):\n",
    "#                 print(\"L_ab is nan.\")\n",
    "        # if torch.isnan(L_inverse):\n",
    "        #     print(\"L_inverse is nan.\")\n",
    "        if args.model == \"DeepMeshFlow_s\":\n",
    "            im1_warp, im2_warp, homography_grid, homography_grid_inv, mask1_warp, mask2_warp = model(input_tensor)\n",
    "            raw_img1 = input_tensor[:, 0:3, :, :]\n",
    "            raw_img2 = input_tensor[:, 3:6, :, :]\n",
    "            loss1 = F.l1_loss(im1_warp[im1_warp.bool()], raw_img2[im1_warp.bool()])\n",
    "            loss2 = F.l1_loss(raw_img1[im2_warp.bool()], im2_warp[im2_warp.bool()])\n",
    "            loss = loss1 + loss2\n",
    "        if torch.isnan(loss):\n",
    "            break_flag = 1\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss[0] += loss.item()\n",
    "        running_loss[1] += loss1.item()\n",
    "        running_loss[2] += loss2.item()\n",
    "        # running_loss[3] += L_ab.item()\n",
    "        # running_loss[4] += L_inverse.item()\n",
    "        print_loss += loss.item()\n",
    "        print_loss_items[0] += loss1.item()\n",
    "        print_loss_items[1] += loss2.item()\n",
    "        # print_loss_items[2] += L_ab.item()\n",
    "        # print_loss_items[3] += L_inverse.item()\n",
    "\n",
    "        if (i + 1) % print_every_iter == 0:\n",
    "            logger.info('[{}, {}] running_loss = {:.5f}, loss1 = {:.5f}, loss2 = {:.5f}, L_ab = {:.5f}, L_inversable = {:.5f}, learning_rate = {:.7f}'. \\\n",
    "                format(epoch + 1, i + 1, print_loss / print_every_iter, print_loss_items[0] / print_every_iter, print_loss_items[1] / print_every_iter, print_loss_items[2] / print_every_iter, print_loss_items[3] / print_every_iter,\\\n",
    "                    optimizer.state_dict()['param_groups'][0]['lr']))                \n",
    "            print_loss = 0.\n",
    "            for t in range(len(print_loss_items)):\n",
    "                print_loss_items[t] = 0\n",
    "#         if (i + 1) % record_every_iter == 0:\n",
    "#             writer.add_scalar(\"loss/Ln_ab\", running_loss[1] / record_every_iter, tb_index)\n",
    "#             writer.add_scalar(\"loss/Ln_ba\", running_loss[2] / record_every_iter, tb_index)\n",
    "#             writer.add_scalar(\"loss/L\", running_loss[3] / record_every_iter, tb_index)\n",
    "#             writer.add_scalar(\"loss/L_inversable\", running_loss[4] / record_every_iter, tb_index)\n",
    "#             writer.add_scalar(\"loss/loss_all\", running_loss[0] / record_every_iter, tb_index)\n",
    "#             tb_index += 1\n",
    "#             for j in range(len(running_loss)):\n",
    "#                 running_loss[j] = 0.\n",
    "    scheduler.step()\n",
    "    if break_flag:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2108, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(ssim_value1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "       device='cuda:1', grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im1_warp[torch.isnan(im1_warp)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]]],\n",
       "\n",
       "\n",
       "        [[[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]]],\n",
       "\n",
       "\n",
       "        [[[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]]],\n",
       "\n",
       "\n",
       "        [[[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]]],\n",
       "\n",
       "\n",
       "        [[[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:1',\n",
       "       grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature1_warp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warped_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from dataset import align_dataset\n",
    "\n",
    "from config.option import args\n",
    "from utils.get_logger import Get_logger\n",
    "from utils.metrics import calc_psnr\n",
    "from utils.solve_DLT import solve_mesh_flow_DLT, spatial_transform_by_grid\n",
    "from utils.toolkit import tensor2img\n",
    "from pytorch_msssim.ssim import ssim\n",
    "from models.deep_mesh_flow import DeepMeshFlow\n",
    "from models.deep_mesh_flow_s import DeepMeshFlow_s\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "# initialize logging file\n",
    "# logger = Get_logger(os.path.join(\"log\", args.exp_name + \".log\"))\n",
    "# logger.info(\"Parameters Setting:\")\n",
    "# for i in args.__dict__:\n",
    "#     logger.info(\"{param_name}: {param_value}\".format(param_name=i, param_value=args.__dict__[i]))\n",
    "model_list = {\"DeepMeshFlow\": DeepMeshFlow, \"DeepMeshFlow_s\": DeepMeshFlow_s}\n",
    "model = model_list[args.model](args=args, device=device)\n",
    "model = model.to(device)\n",
    "train_dataset = align_dataset(args=args)\n",
    "# val_dataset = align_dataset(args=args, validation=True)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers)\n",
    "# val_loader = DataLoader(dataset=val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.num_workers)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer=optimizer, step_size=1, gamma=args.weight_decay)\n",
    "\n",
    "# result save\n",
    "# writer = SummaryWriter(os.path.join(\"runs\", args.exp_name))\n",
    "model_save_path = os.path.join(\"checkpoints\", args.exp_name)\n",
    "if not os.path.isdir(model_save_path):\n",
    "    os.mkdir(model_save_path)\n",
    "\n",
    "print_every_iter = args.print_every_iter\n",
    "record_every_iter = args.record_every_iter\n",
    "save_every_epoch = args.save_every_epoch\n",
    "\n",
    "epochs = args.epoch\n",
    "start_epoch = 0\n",
    "tb_index = 0 + ((len(train_dataset) // args.batch_size) // args.record_every_iter) * start_epoch\n",
    "# patch_size = (args.image_size[0] // (args.mesh_size_3[0] - 1), args.image_size[1] // (args.mesh_size_3[1] - 1))\n",
    "# logger.info(\"Training start.\")\n",
    "# if args.resume:\n",
    "loaded = torch.load(\"checkpoints/test/align_latest.pth\")\n",
    "model.load_state_dict(loaded['state_dict'])\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=loaded['lr'])\n",
    "# start_epoch = loaded['epoch']\n",
    "# for epoch in range(start_epoch, epochs):\n",
    "# model.eval()\n",
    "# running_loss = [0., 0., 0., 0., 0., 0.]\n",
    "# print_loss = 0.\n",
    "# print_loss_items = [0., 0., 0., 0., 0.,]\n",
    "# with torch.no_grad():\n",
    "#     for i, sample_batch in enumerate(train_loader):\n",
    "#         print(i)\n",
    "#         input_tensor = sample_batch[0].to(device)\n",
    "#         img_names = sample_batch[1]\n",
    "#         raw_img1 = input_tensor[:, 0:3, :, :]\n",
    "#         raw_img2 = input_tensor[:, 3:6, :, :]\n",
    "#         # loss for DeepMeshFlow\n",
    "#         if args.model == \"DeepMeshFlow\":\n",
    "#             feature1_warp, feature2_warp, feature1_orig, feature2_orig, mask1_orig, mask2_orig, mask1_warp, mask2_warp, \\\n",
    "#             homography_grid, homography_grid_inv, warped_grid, warped_grid_inv, mesh_out, mesh_out_inv, im1_warp, im2_warp, ones_mask1_warp, ones_mask2_warp = model(input_tensor)\n",
    "\n",
    "#             ln = torch.sum(mask1_warp * mask2_orig * torch.abs(feature2_orig - feature1_warp)) / torch.sum(mask1_warp * mask2_orig)\n",
    "#             raw_im1_warp = spatial_transform_by_grid(raw_img1, warped_grid, device=device)\n",
    "#             raw_im2_warp = spatial_transform_by_grid(raw_img2, warped_grid_inv, device=device)\n",
    "#             loss1 = F.l1_loss(raw_im1_warp[ones_mask1_warp.bool()], raw_img2[ones_mask1_warp.bool()])\n",
    "#             loss2 = F.l1_loss(raw_img1[ones_mask2_warp.bool()], raw_im2_warp[ones_mask2_warp.bool()])\n",
    "\n",
    "#             ln_inv = torch.sum(mask1_orig * mask2_warp * torch.abs(feature1_orig - feature2_warp)) / torch.sum(mask1_orig * mask2_warp)\n",
    "#             # TODO: Does L ab loss should be normalized ?\n",
    "#             L_ab = -1 * torch.abs(feature1_orig - feature2_orig).mean()\n",
    "#             # Identity = torch.eye(3, dtype=torch.float, device=device).unsqueeze(0).expand(homography_grid.shape[0], -1, -1)\n",
    "#             # L_inverse = torch.mean(torch.abs(torch.matmul(homography_grid, homography_grid_inv) - Identity))\n",
    "#             # loss = ln + ln_inv + args.loss_weight_lambda * L_ab + args.loss_weight_mu * L_inverse\n",
    "#             loss = ln + ln_inv + args.loss_weight_lambda * L_ab + 2 * loss1 + 2 * loss2\n",
    "#         if torch.isnan(ln):\n",
    "#             print(\"ln is nan.\")\n",
    "#         if torch.isnan(ln_inv):\n",
    "#             print(\"ln_inv is nan.\")\n",
    "#         if torch.isnan(L_ab):\n",
    "#             print(\"L_ab is nan.\")\n",
    "#         # if torch.isnan(L_inverse):\n",
    "#         #     print(\"L_inverse is nan.\")\n",
    "#         if torch.isnan(loss):\n",
    "#             exit()\n",
    "#         if args.model == \"DeepMeshFlow_s\":\n",
    "#             im1_warp, im2_warp, homography_grid, homography_grid_inv, mask1_warp, mask2_warp = model(input_tensor)\n",
    "#             raw_img1 = input_tensor[:, 0:3, :, :]\n",
    "#             raw_img2 = input_tensor[:, 3:6, :, :]\n",
    "#             loss1 = F.l1_loss(im1_warp[mask1_warp.bool()], raw_img2[mask1_warp.bool()])\n",
    "#             loss2 = F.l1_loss(raw_img1[mask2_warp.bool()], im2_warp[mask2_warp.bool()])\n",
    "#             loss = loss1 + loss2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['feature_extractor.extractor.0.weight', 'feature_extractor.extractor.1.weight', 'feature_extractor.extractor.1.bias', 'feature_extractor.extractor.1.running_mean', 'feature_extractor.extractor.1.running_var', 'feature_extractor.extractor.1.num_batches_tracked', 'feature_extractor.extractor.3.weight', 'feature_extractor.extractor.4.weight', 'feature_extractor.extractor.4.bias', 'feature_extractor.extractor.4.running_mean', 'feature_extractor.extractor.4.running_var', 'feature_extractor.extractor.4.num_batches_tracked', 'feature_extractor.extractor.6.weight', 'feature_extractor.extractor.7.weight', 'feature_extractor.extractor.7.bias', 'feature_extractor.extractor.7.running_mean', 'feature_extractor.extractor.7.running_var', 'feature_extractor.extractor.7.num_batches_tracked', 'mask_predictor.predictor.0.weight', 'mask_predictor.predictor.1.weight', 'mask_predictor.predictor.1.bias', 'mask_predictor.predictor.1.running_mean', 'mask_predictor.predictor.1.running_var', 'mask_predictor.predictor.1.num_batches_tracked', 'mask_predictor.predictor.3.weight', 'mask_predictor.predictor.4.weight', 'mask_predictor.predictor.4.bias', 'mask_predictor.predictor.4.running_mean', 'mask_predictor.predictor.4.running_var', 'mask_predictor.predictor.4.num_batches_tracked', 'mask_predictor.predictor.6.weight', 'mask_predictor.predictor.7.weight', 'mask_predictor.predictor.7.bias', 'mask_predictor.predictor.7.running_mean', 'mask_predictor.predictor.7.running_var', 'mask_predictor.predictor.7.num_batches_tracked', 'mask_predictor.predictor.9.weight', 'mask_predictor.predictor.10.weight', 'mask_predictor.predictor.10.bias', 'mask_predictor.predictor.10.running_mean', 'mask_predictor.predictor.10.running_var', 'mask_predictor.predictor.10.num_batches_tracked', 'mask_predictor.predictor.12.weight', 'mask_predictor.predictor.13.weight', 'mask_predictor.predictor.13.bias', 'mask_predictor.predictor.13.running_mean', 'mask_predictor.predictor.13.running_var', 'mask_predictor.predictor.13.num_batches_tracked', 'estimator_body.conv1.0.weight', 'estimator_body.conv1.1.weight', 'estimator_body.conv1.1.bias', 'estimator_body.conv1.1.running_mean', 'estimator_body.conv1.1.running_var', 'estimator_body.conv1.1.num_batches_tracked', 'estimator_body.conv2_x.0.residual_function.0.weight', 'estimator_body.conv2_x.0.residual_function.1.weight', 'estimator_body.conv2_x.0.residual_function.1.bias', 'estimator_body.conv2_x.0.residual_function.1.running_mean', 'estimator_body.conv2_x.0.residual_function.1.running_var', 'estimator_body.conv2_x.0.residual_function.1.num_batches_tracked', 'estimator_body.conv2_x.0.residual_function.3.weight', 'estimator_body.conv2_x.0.residual_function.4.weight', 'estimator_body.conv2_x.0.residual_function.4.bias', 'estimator_body.conv2_x.0.residual_function.4.running_mean', 'estimator_body.conv2_x.0.residual_function.4.running_var', 'estimator_body.conv2_x.0.residual_function.4.num_batches_tracked', 'estimator_body.conv2_x.1.residual_function.0.weight', 'estimator_body.conv2_x.1.residual_function.1.weight', 'estimator_body.conv2_x.1.residual_function.1.bias', 'estimator_body.conv2_x.1.residual_function.1.running_mean', 'estimator_body.conv2_x.1.residual_function.1.running_var', 'estimator_body.conv2_x.1.residual_function.1.num_batches_tracked', 'estimator_body.conv2_x.1.residual_function.3.weight', 'estimator_body.conv2_x.1.residual_function.4.weight', 'estimator_body.conv2_x.1.residual_function.4.bias', 'estimator_body.conv2_x.1.residual_function.4.running_mean', 'estimator_body.conv2_x.1.residual_function.4.running_var', 'estimator_body.conv2_x.1.residual_function.4.num_batches_tracked', 'estimator_body.conv2_x.2.residual_function.0.weight', 'estimator_body.conv2_x.2.residual_function.1.weight', 'estimator_body.conv2_x.2.residual_function.1.bias', 'estimator_body.conv2_x.2.residual_function.1.running_mean', 'estimator_body.conv2_x.2.residual_function.1.running_var', 'estimator_body.conv2_x.2.residual_function.1.num_batches_tracked', 'estimator_body.conv2_x.2.residual_function.3.weight', 'estimator_body.conv2_x.2.residual_function.4.weight', 'estimator_body.conv2_x.2.residual_function.4.bias', 'estimator_body.conv2_x.2.residual_function.4.running_mean', 'estimator_body.conv2_x.2.residual_function.4.running_var', 'estimator_body.conv2_x.2.residual_function.4.num_batches_tracked', 'estimator_body.conv3_x.0.residual_function.0.weight', 'estimator_body.conv3_x.0.residual_function.1.weight', 'estimator_body.conv3_x.0.residual_function.1.bias', 'estimator_body.conv3_x.0.residual_function.1.running_mean', 'estimator_body.conv3_x.0.residual_function.1.running_var', 'estimator_body.conv3_x.0.residual_function.1.num_batches_tracked', 'estimator_body.conv3_x.0.residual_function.3.weight', 'estimator_body.conv3_x.0.residual_function.4.weight', 'estimator_body.conv3_x.0.residual_function.4.bias', 'estimator_body.conv3_x.0.residual_function.4.running_mean', 'estimator_body.conv3_x.0.residual_function.4.running_var', 'estimator_body.conv3_x.0.residual_function.4.num_batches_tracked', 'estimator_body.conv3_x.0.shortcut.0.weight', 'estimator_body.conv3_x.0.shortcut.1.weight', 'estimator_body.conv3_x.0.shortcut.1.bias', 'estimator_body.conv3_x.0.shortcut.1.running_mean', 'estimator_body.conv3_x.0.shortcut.1.running_var', 'estimator_body.conv3_x.0.shortcut.1.num_batches_tracked', 'estimator_body.conv3_x.1.residual_function.0.weight', 'estimator_body.conv3_x.1.residual_function.1.weight', 'estimator_body.conv3_x.1.residual_function.1.bias', 'estimator_body.conv3_x.1.residual_function.1.running_mean', 'estimator_body.conv3_x.1.residual_function.1.running_var', 'estimator_body.conv3_x.1.residual_function.1.num_batches_tracked', 'estimator_body.conv3_x.1.residual_function.3.weight', 'estimator_body.conv3_x.1.residual_function.4.weight', 'estimator_body.conv3_x.1.residual_function.4.bias', 'estimator_body.conv3_x.1.residual_function.4.running_mean', 'estimator_body.conv3_x.1.residual_function.4.running_var', 'estimator_body.conv3_x.1.residual_function.4.num_batches_tracked', 'estimator_body.conv3_x.2.residual_function.0.weight', 'estimator_body.conv3_x.2.residual_function.1.weight', 'estimator_body.conv3_x.2.residual_function.1.bias', 'estimator_body.conv3_x.2.residual_function.1.running_mean', 'estimator_body.conv3_x.2.residual_function.1.running_var', 'estimator_body.conv3_x.2.residual_function.1.num_batches_tracked', 'estimator_body.conv3_x.2.residual_function.3.weight', 'estimator_body.conv3_x.2.residual_function.4.weight', 'estimator_body.conv3_x.2.residual_function.4.bias', 'estimator_body.conv3_x.2.residual_function.4.running_mean', 'estimator_body.conv3_x.2.residual_function.4.running_var', 'estimator_body.conv3_x.2.residual_function.4.num_batches_tracked', 'estimator_body.conv3_x.3.residual_function.0.weight', 'estimator_body.conv3_x.3.residual_function.1.weight', 'estimator_body.conv3_x.3.residual_function.1.bias', 'estimator_body.conv3_x.3.residual_function.1.running_mean', 'estimator_body.conv3_x.3.residual_function.1.running_var', 'estimator_body.conv3_x.3.residual_function.1.num_batches_tracked', 'estimator_body.conv3_x.3.residual_function.3.weight', 'estimator_body.conv3_x.3.residual_function.4.weight', 'estimator_body.conv3_x.3.residual_function.4.bias', 'estimator_body.conv3_x.3.residual_function.4.running_mean', 'estimator_body.conv3_x.3.residual_function.4.running_var', 'estimator_body.conv3_x.3.residual_function.4.num_batches_tracked', 'estimator_body.conv4_x.0.residual_function.0.weight', 'estimator_body.conv4_x.0.residual_function.1.weight', 'estimator_body.conv4_x.0.residual_function.1.bias', 'estimator_body.conv4_x.0.residual_function.1.running_mean', 'estimator_body.conv4_x.0.residual_function.1.running_var', 'estimator_body.conv4_x.0.residual_function.1.num_batches_tracked', 'estimator_body.conv4_x.0.residual_function.3.weight', 'estimator_body.conv4_x.0.residual_function.4.weight', 'estimator_body.conv4_x.0.residual_function.4.bias', 'estimator_body.conv4_x.0.residual_function.4.running_mean', 'estimator_body.conv4_x.0.residual_function.4.running_var', 'estimator_body.conv4_x.0.residual_function.4.num_batches_tracked', 'estimator_body.conv4_x.0.shortcut.0.weight', 'estimator_body.conv4_x.0.shortcut.1.weight', 'estimator_body.conv4_x.0.shortcut.1.bias', 'estimator_body.conv4_x.0.shortcut.1.running_mean', 'estimator_body.conv4_x.0.shortcut.1.running_var', 'estimator_body.conv4_x.0.shortcut.1.num_batches_tracked', 'estimator_body.conv4_x.1.residual_function.0.weight', 'estimator_body.conv4_x.1.residual_function.1.weight', 'estimator_body.conv4_x.1.residual_function.1.bias', 'estimator_body.conv4_x.1.residual_function.1.running_mean', 'estimator_body.conv4_x.1.residual_function.1.running_var', 'estimator_body.conv4_x.1.residual_function.1.num_batches_tracked', 'estimator_body.conv4_x.1.residual_function.3.weight', 'estimator_body.conv4_x.1.residual_function.4.weight', 'estimator_body.conv4_x.1.residual_function.4.bias', 'estimator_body.conv4_x.1.residual_function.4.running_mean', 'estimator_body.conv4_x.1.residual_function.4.running_var', 'estimator_body.conv4_x.1.residual_function.4.num_batches_tracked', 'estimator_body.conv4_x.2.residual_function.0.weight', 'estimator_body.conv4_x.2.residual_function.1.weight', 'estimator_body.conv4_x.2.residual_function.1.bias', 'estimator_body.conv4_x.2.residual_function.1.running_mean', 'estimator_body.conv4_x.2.residual_function.1.running_var', 'estimator_body.conv4_x.2.residual_function.1.num_batches_tracked', 'estimator_body.conv4_x.2.residual_function.3.weight', 'estimator_body.conv4_x.2.residual_function.4.weight', 'estimator_body.conv4_x.2.residual_function.4.bias', 'estimator_body.conv4_x.2.residual_function.4.running_mean', 'estimator_body.conv4_x.2.residual_function.4.running_var', 'estimator_body.conv4_x.2.residual_function.4.num_batches_tracked', 'estimator_body.conv4_x.3.residual_function.0.weight', 'estimator_body.conv4_x.3.residual_function.1.weight', 'estimator_body.conv4_x.3.residual_function.1.bias', 'estimator_body.conv4_x.3.residual_function.1.running_mean', 'estimator_body.conv4_x.3.residual_function.1.running_var', 'estimator_body.conv4_x.3.residual_function.1.num_batches_tracked', 'estimator_body.conv4_x.3.residual_function.3.weight', 'estimator_body.conv4_x.3.residual_function.4.weight', 'estimator_body.conv4_x.3.residual_function.4.bias', 'estimator_body.conv4_x.3.residual_function.4.running_mean', 'estimator_body.conv4_x.3.residual_function.4.running_var', 'estimator_body.conv4_x.3.residual_function.4.num_batches_tracked', 'estimator_body.conv4_x.4.residual_function.0.weight', 'estimator_body.conv4_x.4.residual_function.1.weight', 'estimator_body.conv4_x.4.residual_function.1.bias', 'estimator_body.conv4_x.4.residual_function.1.running_mean', 'estimator_body.conv4_x.4.residual_function.1.running_var', 'estimator_body.conv4_x.4.residual_function.1.num_batches_tracked', 'estimator_body.conv4_x.4.residual_function.3.weight', 'estimator_body.conv4_x.4.residual_function.4.weight', 'estimator_body.conv4_x.4.residual_function.4.bias', 'estimator_body.conv4_x.4.residual_function.4.running_mean', 'estimator_body.conv4_x.4.residual_function.4.running_var', 'estimator_body.conv4_x.4.residual_function.4.num_batches_tracked', 'estimator_body.conv4_x.5.residual_function.0.weight', 'estimator_body.conv4_x.5.residual_function.1.weight', 'estimator_body.conv4_x.5.residual_function.1.bias', 'estimator_body.conv4_x.5.residual_function.1.running_mean', 'estimator_body.conv4_x.5.residual_function.1.running_var', 'estimator_body.conv4_x.5.residual_function.1.num_batches_tracked', 'estimator_body.conv4_x.5.residual_function.3.weight', 'estimator_body.conv4_x.5.residual_function.4.weight', 'estimator_body.conv4_x.5.residual_function.4.bias', 'estimator_body.conv4_x.5.residual_function.4.running_mean', 'estimator_body.conv4_x.5.residual_function.4.running_var', 'estimator_body.conv4_x.5.residual_function.4.num_batches_tracked', 'estimator_body.conv5_x.0.residual_function.0.weight', 'estimator_body.conv5_x.0.residual_function.1.weight', 'estimator_body.conv5_x.0.residual_function.1.bias', 'estimator_body.conv5_x.0.residual_function.1.running_mean', 'estimator_body.conv5_x.0.residual_function.1.running_var', 'estimator_body.conv5_x.0.residual_function.1.num_batches_tracked', 'estimator_body.conv5_x.0.residual_function.3.weight', 'estimator_body.conv5_x.0.residual_function.4.weight', 'estimator_body.conv5_x.0.residual_function.4.bias', 'estimator_body.conv5_x.0.residual_function.4.running_mean', 'estimator_body.conv5_x.0.residual_function.4.running_var', 'estimator_body.conv5_x.0.residual_function.4.num_batches_tracked', 'estimator_body.conv5_x.0.shortcut.0.weight', 'estimator_body.conv5_x.0.shortcut.1.weight', 'estimator_body.conv5_x.0.shortcut.1.bias', 'estimator_body.conv5_x.0.shortcut.1.running_mean', 'estimator_body.conv5_x.0.shortcut.1.running_var', 'estimator_body.conv5_x.0.shortcut.1.num_batches_tracked', 'estimator_body.conv5_x.1.residual_function.0.weight', 'estimator_body.conv5_x.1.residual_function.1.weight', 'estimator_body.conv5_x.1.residual_function.1.bias', 'estimator_body.conv5_x.1.residual_function.1.running_mean', 'estimator_body.conv5_x.1.residual_function.1.running_var', 'estimator_body.conv5_x.1.residual_function.1.num_batches_tracked', 'estimator_body.conv5_x.1.residual_function.3.weight', 'estimator_body.conv5_x.1.residual_function.4.weight', 'estimator_body.conv5_x.1.residual_function.4.bias', 'estimator_body.conv5_x.1.residual_function.4.running_mean', 'estimator_body.conv5_x.1.residual_function.4.running_var', 'estimator_body.conv5_x.1.residual_function.4.num_batches_tracked', 'estimator_body.conv5_x.2.residual_function.0.weight', 'estimator_body.conv5_x.2.residual_function.1.weight', 'estimator_body.conv5_x.2.residual_function.1.bias', 'estimator_body.conv5_x.2.residual_function.1.running_mean', 'estimator_body.conv5_x.2.residual_function.1.running_var', 'estimator_body.conv5_x.2.residual_function.1.num_batches_tracked', 'estimator_body.conv5_x.2.residual_function.3.weight', 'estimator_body.conv5_x.2.residual_function.4.weight', 'estimator_body.conv5_x.2.residual_function.4.bias', 'estimator_body.conv5_x.2.residual_function.4.running_mean', 'estimator_body.conv5_x.2.residual_function.4.running_var', 'estimator_body.conv5_x.2.residual_function.4.num_batches_tracked', 'estimator_head_0.fc1.0.weight', 'estimator_head_0.fc1.0.bias', 'estimator_head_0.fc3.0.weight', 'estimator_head_0.fc3.0.bias', 'estimator_head_1.fc1.0.weight', 'estimator_head_1.fc1.0.bias', 'estimator_head_1.fc3.0.weight', 'estimator_head_1.fc3.0.bias', 'estimator_head_2.fc1.0.weight', 'estimator_head_2.fc1.0.bias', 'estimator_head_2.fc3.0.weight', 'estimator_head_2.fc3.0.bias', 'mesh_selector.features.conv1.0.weight', 'mesh_selector.features.conv1.1.weight', 'mesh_selector.features.conv1.1.bias', 'mesh_selector.features.conv1.1.running_mean', 'mesh_selector.features.conv1.1.running_var', 'mesh_selector.features.conv1.1.num_batches_tracked', 'mesh_selector.features.conv2_x.0.residual_function.0.weight', 'mesh_selector.features.conv2_x.0.residual_function.1.weight', 'mesh_selector.features.conv2_x.0.residual_function.1.bias', 'mesh_selector.features.conv2_x.0.residual_function.1.running_mean', 'mesh_selector.features.conv2_x.0.residual_function.1.running_var', 'mesh_selector.features.conv2_x.0.residual_function.1.num_batches_tracked', 'mesh_selector.features.conv2_x.0.residual_function.3.weight', 'mesh_selector.features.conv2_x.0.residual_function.4.weight', 'mesh_selector.features.conv2_x.0.residual_function.4.bias', 'mesh_selector.features.conv2_x.0.residual_function.4.running_mean', 'mesh_selector.features.conv2_x.0.residual_function.4.running_var', 'mesh_selector.features.conv2_x.0.residual_function.4.num_batches_tracked', 'mesh_selector.features.conv2_x.1.residual_function.0.weight', 'mesh_selector.features.conv2_x.1.residual_function.1.weight', 'mesh_selector.features.conv2_x.1.residual_function.1.bias', 'mesh_selector.features.conv2_x.1.residual_function.1.running_mean', 'mesh_selector.features.conv2_x.1.residual_function.1.running_var', 'mesh_selector.features.conv2_x.1.residual_function.1.num_batches_tracked', 'mesh_selector.features.conv2_x.1.residual_function.3.weight', 'mesh_selector.features.conv2_x.1.residual_function.4.weight', 'mesh_selector.features.conv2_x.1.residual_function.4.bias', 'mesh_selector.features.conv2_x.1.residual_function.4.running_mean', 'mesh_selector.features.conv2_x.1.residual_function.4.running_var', 'mesh_selector.features.conv2_x.1.residual_function.4.num_batches_tracked', 'mesh_selector.features.conv3_x.0.residual_function.0.weight', 'mesh_selector.features.conv3_x.0.residual_function.1.weight', 'mesh_selector.features.conv3_x.0.residual_function.1.bias', 'mesh_selector.features.conv3_x.0.residual_function.1.running_mean', 'mesh_selector.features.conv3_x.0.residual_function.1.running_var', 'mesh_selector.features.conv3_x.0.residual_function.1.num_batches_tracked', 'mesh_selector.features.conv3_x.0.residual_function.3.weight', 'mesh_selector.features.conv3_x.0.residual_function.4.weight', 'mesh_selector.features.conv3_x.0.residual_function.4.bias', 'mesh_selector.features.conv3_x.0.residual_function.4.running_mean', 'mesh_selector.features.conv3_x.0.residual_function.4.running_var', 'mesh_selector.features.conv3_x.0.residual_function.4.num_batches_tracked', 'mesh_selector.features.conv3_x.0.shortcut.0.weight', 'mesh_selector.features.conv3_x.0.shortcut.1.weight', 'mesh_selector.features.conv3_x.0.shortcut.1.bias', 'mesh_selector.features.conv3_x.0.shortcut.1.running_mean', 'mesh_selector.features.conv3_x.0.shortcut.1.running_var', 'mesh_selector.features.conv3_x.0.shortcut.1.num_batches_tracked', 'mesh_selector.features.conv3_x.1.residual_function.0.weight', 'mesh_selector.features.conv3_x.1.residual_function.1.weight', 'mesh_selector.features.conv3_x.1.residual_function.1.bias', 'mesh_selector.features.conv3_x.1.residual_function.1.running_mean', 'mesh_selector.features.conv3_x.1.residual_function.1.running_var', 'mesh_selector.features.conv3_x.1.residual_function.1.num_batches_tracked', 'mesh_selector.features.conv3_x.1.residual_function.3.weight', 'mesh_selector.features.conv3_x.1.residual_function.4.weight', 'mesh_selector.features.conv3_x.1.residual_function.4.bias', 'mesh_selector.features.conv3_x.1.residual_function.4.running_mean', 'mesh_selector.features.conv3_x.1.residual_function.4.running_var', 'mesh_selector.features.conv3_x.1.residual_function.4.num_batches_tracked', 'mesh_selector.features.conv4_x.0.residual_function.0.weight', 'mesh_selector.features.conv4_x.0.residual_function.1.weight', 'mesh_selector.features.conv4_x.0.residual_function.1.bias', 'mesh_selector.features.conv4_x.0.residual_function.1.running_mean', 'mesh_selector.features.conv4_x.0.residual_function.1.running_var', 'mesh_selector.features.conv4_x.0.residual_function.1.num_batches_tracked', 'mesh_selector.features.conv4_x.0.residual_function.3.weight', 'mesh_selector.features.conv4_x.0.residual_function.4.weight', 'mesh_selector.features.conv4_x.0.residual_function.4.bias', 'mesh_selector.features.conv4_x.0.residual_function.4.running_mean', 'mesh_selector.features.conv4_x.0.residual_function.4.running_var', 'mesh_selector.features.conv4_x.0.residual_function.4.num_batches_tracked', 'mesh_selector.features.conv4_x.0.shortcut.0.weight', 'mesh_selector.features.conv4_x.0.shortcut.1.weight', 'mesh_selector.features.conv4_x.0.shortcut.1.bias', 'mesh_selector.features.conv4_x.0.shortcut.1.running_mean', 'mesh_selector.features.conv4_x.0.shortcut.1.running_var', 'mesh_selector.features.conv4_x.0.shortcut.1.num_batches_tracked', 'mesh_selector.features.conv4_x.1.residual_function.0.weight', 'mesh_selector.features.conv4_x.1.residual_function.1.weight', 'mesh_selector.features.conv4_x.1.residual_function.1.bias', 'mesh_selector.features.conv4_x.1.residual_function.1.running_mean', 'mesh_selector.features.conv4_x.1.residual_function.1.running_var', 'mesh_selector.features.conv4_x.1.residual_function.1.num_batches_tracked', 'mesh_selector.features.conv4_x.1.residual_function.3.weight', 'mesh_selector.features.conv4_x.1.residual_function.4.weight', 'mesh_selector.features.conv4_x.1.residual_function.4.bias', 'mesh_selector.features.conv4_x.1.residual_function.4.running_mean', 'mesh_selector.features.conv4_x.1.residual_function.4.running_var', 'mesh_selector.features.conv4_x.1.residual_function.4.num_batches_tracked', 'mesh_selector.features.conv5_x.0.residual_function.0.weight', 'mesh_selector.features.conv5_x.0.residual_function.1.weight', 'mesh_selector.features.conv5_x.0.residual_function.1.bias', 'mesh_selector.features.conv5_x.0.residual_function.1.running_mean', 'mesh_selector.features.conv5_x.0.residual_function.1.running_var', 'mesh_selector.features.conv5_x.0.residual_function.1.num_batches_tracked', 'mesh_selector.features.conv5_x.0.residual_function.3.weight', 'mesh_selector.features.conv5_x.0.residual_function.4.weight', 'mesh_selector.features.conv5_x.0.residual_function.4.bias', 'mesh_selector.features.conv5_x.0.residual_function.4.running_mean', 'mesh_selector.features.conv5_x.0.residual_function.4.running_var', 'mesh_selector.features.conv5_x.0.residual_function.4.num_batches_tracked', 'mesh_selector.features.conv5_x.0.shortcut.0.weight', 'mesh_selector.features.conv5_x.0.shortcut.1.weight', 'mesh_selector.features.conv5_x.0.shortcut.1.bias', 'mesh_selector.features.conv5_x.0.shortcut.1.running_mean', 'mesh_selector.features.conv5_x.0.shortcut.1.running_var', 'mesh_selector.features.conv5_x.0.shortcut.1.num_batches_tracked', 'mesh_selector.features.conv5_x.1.residual_function.0.weight', 'mesh_selector.features.conv5_x.1.residual_function.1.weight', 'mesh_selector.features.conv5_x.1.residual_function.1.bias', 'mesh_selector.features.conv5_x.1.residual_function.1.running_mean', 'mesh_selector.features.conv5_x.1.residual_function.1.running_var', 'mesh_selector.features.conv5_x.1.residual_function.1.num_batches_tracked', 'mesh_selector.features.conv5_x.1.residual_function.3.weight', 'mesh_selector.features.conv5_x.1.residual_function.4.weight', 'mesh_selector.features.conv5_x.1.residual_function.4.bias', 'mesh_selector.features.conv5_x.1.residual_function.4.running_mean', 'mesh_selector.features.conv5_x.1.residual_function.4.running_var', 'mesh_selector.features.conv5_x.1.residual_function.4.num_batches_tracked', 'mesh_selector.fc.weight', 'mesh_selector.fc.bias'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wyq/anaconda3/envs/pytorch/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "debug here 1: (tensor([], device='cuda:1', dtype=torch.int64), tensor([], device='cuda:1', dtype=torch.int64), tensor([], device='cuda:1', dtype=torch.int64), tensor([], device='cuda:1', dtype=torch.int64), tensor([], device='cuda:1', dtype=torch.int64))\n",
      "debug here 2: (tensor([], device='cuda:1', dtype=torch.int64), tensor([], device='cuda:1', dtype=torch.int64), tensor([], device='cuda:1', dtype=torch.int64), tensor([], device='cuda:1', dtype=torch.int64), tensor([], device='cuda:1', dtype=torch.int64))\n",
      "debug here 3: (tensor([0, 0], device='cuda:1'), tensor([97, 97], device='cuda:1'), tensor([75, 75], device='cuda:1'), tensor([0, 1], device='cuda:1'))\n",
      "debug here 4: tensor([[1.],\n",
      "        [1.]], device='cuda:1')\n",
      "ln is nan.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from dataset import align_dataset\n",
    "\n",
    "from config.option import args\n",
    "from utils.get_logger import Get_logger\n",
    "from utils.metrics import calc_psnr\n",
    "from utils.solve_DLT import solve_mesh_flow_DLT, spatial_transform_by_grid\n",
    "from utils.toolkit import tensor2img\n",
    "from pytorch_msssim.ssim import ssim\n",
    "from models.deep_mesh_flow import DeepMeshFlow\n",
    "from models.deep_mesh_flow_s import DeepMeshFlow_s\n",
    "\n",
    "import os\n",
    "\n",
    "if args.cpu:\n",
    "    device = torch.device(\"cpu\")\n",
    "else:\n",
    "    # os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu\n",
    "    device = torch.device(\"cuda:{gpu_num}\".format(gpu_num=args.gpu))    \n",
    "\n",
    "# initialize logging file\n",
    "# logger = Get_logger(os.path.join(\"log\", args.exp_name + \".log\"))\n",
    "# logger.info(\"Parameters Setting:\")\n",
    "# for i in args.__dict__:\n",
    "#     logger.info(\"{param_name}: {param_value}\".format(param_name=i, param_value=args.__dict__[i]))\n",
    "model_list = {\"DeepMeshFlow\": DeepMeshFlow, \"DeepMeshFlow_s\": DeepMeshFlow_s}\n",
    "model = model_list[args.model](args=args, device=device)\n",
    "model = model.to(device)\n",
    "train_dataset = align_dataset(args=args)\n",
    "val_dataset = align_dataset(args=args, validation=True)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.num_workers)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer=optimizer, step_size=1, gamma=args.weight_decay)\n",
    "\n",
    "# result save\n",
    "writer = SummaryWriter(os.path.join(\"runs\", args.exp_name))\n",
    "model_save_path = os.path.join(\"checkpoints\", args.exp_name)\n",
    "if not os.path.isdir(model_save_path):\n",
    "    os.mkdir(model_save_path)\n",
    "\n",
    "print_every_iter = args.print_every_iter\n",
    "record_every_iter = args.record_every_iter\n",
    "save_every_epoch = args.save_every_epoch\n",
    "\n",
    "epochs = args.epoch\n",
    "start_epoch = 0\n",
    "tb_index = 0 + ((len(train_dataset) // args.batch_size) // args.record_every_iter) * start_epoch\n",
    "# patch_size = (args.image_size[0] // (args.mesh_size_3[0] - 1), args.image_size[1] // (args.mesh_size_3[1] - 1))\n",
    "# logger.info(\"Training start.\")\n",
    "break_flag = 0\n",
    "if args.resume:\n",
    "    loaded = torch.load(os.path.join(\"checkpoints\", args.exp_name, args.model_name + \"_latest.pth\"))\n",
    "    model.load_state_dict(loaded['state_dict'])\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=loaded['lr'])\n",
    "    start_epoch = loaded['epoch']\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    model.train()\n",
    "    running_loss = [0., 0., 0., 0., 0., 0.]\n",
    "    print_loss = 0.\n",
    "    print_loss_items = [0., 0., 0., 0., 0.,]\n",
    "    for i, sample_batch in enumerate(train_loader):\n",
    "        input_tensor = sample_batch[0].to(device)\n",
    "        img_names = sample_batch[1]\n",
    "        raw_img1 = input_tensor[:, 0:3, :, :]\n",
    "        raw_img2 = input_tensor[:, 3:6, :, :]\n",
    "        # loss for DeepMeshFlow\n",
    "        if args.model == \"DeepMeshFlow\":\n",
    "            feature1_warp, feature2_warp, feature1_orig, feature2_orig, mask1_orig, mask2_orig, mask1_warp, mask2_warp, \\\n",
    "            homography_grid, homography_grid_inv, warped_grid, warped_grid_inv, mesh_out, mesh_out_inv, im1_warp, im2_warp, ones_mask1_warp, ones_mask2_warp = model(input_tensor)\n",
    "\n",
    "            ln = torch.sum(mask1_warp * mask2_orig * torch.abs(feature2_orig - feature1_warp)) / torch.sum(mask1_warp * mask2_orig)\n",
    "            raw_im1_warp = spatial_transform_by_grid(raw_img1, warped_grid, device=device)\n",
    "            raw_im2_warp = spatial_transform_by_grid(raw_img2, warped_grid_inv, device=device)\n",
    "            loss1 = F.l1_loss(raw_im1_warp[ones_mask1_warp.bool()], raw_img2[ones_mask1_warp.bool()])\n",
    "            loss2 = F.l1_loss(raw_img1[ones_mask2_warp.bool()], raw_im2_warp[ones_mask2_warp.bool()])\n",
    "\n",
    "            ln_inv = torch.sum(mask1_orig * mask2_warp * torch.abs(feature1_orig - feature2_warp)) / torch.sum(mask1_orig * mask2_warp)\n",
    "            # TODO: Does L ab loss should be normalized ?\n",
    "            L_ab = -1 * torch.abs(feature1_orig - feature2_orig).mean()\n",
    "            # Identity = torch.eye(3, dtype=torch.float, device=device).unsqueeze(0).expand(homography_grid.shape[0], -1, -1)\n",
    "            # L_inverse = torch.mean(torch.abs(torch.matmul(homography_grid, homography_grid_inv) - Identity))\n",
    "            # loss = ln + ln_inv + args.loss_weight_lambda * L_ab + args.loss_weight_mu * L_inverse\n",
    "            loss = ln + ln_inv + args.loss_weight_lambda * L_ab + 2 * loss1 + 2 * loss2\n",
    "        if torch.isnan(ln):\n",
    "            print(\"ln is nan.\")\n",
    "        if torch.isnan(ln_inv):\n",
    "            print(\"ln_inv is nan.\")\n",
    "        if torch.isnan(L_ab):\n",
    "            print(\"L_ab is nan.\")\n",
    "        # if torch.isnan(L_inverse):\n",
    "        #     print(\"L_inverse is nan.\")\n",
    "        if torch.isnan(loss):\n",
    "            break_flag = 1\n",
    "            break\n",
    "    if break_flag:\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.load(\"1.pt\")\n",
    "b = torch.load(\"2.pt\")\n",
    "c = torch.load(\"3.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = (a * b).sum(dim=1) / c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 0], device='cuda:1'),\n",
       " tensor([86, 86], device='cuda:1'),\n",
       " tensor([97, 97], device='cuda:1'),\n",
       " tensor([75, 75], device='cuda:1'),\n",
       " tensor([0, 1], device='cuda:1'))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.where(torch.isinf(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([inf, inf], device='cuda:1', grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[torch.where(torch.isnan(a * b))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([], device='cuda:1', dtype=torch.int64),\n",
       " tensor([], device='cuda:1', dtype=torch.int64),\n",
       " tensor([], device='cuda:1', dtype=torch.int64),\n",
       " tensor([], device='cuda:1', dtype=torch.int64),\n",
       " tensor([], device='cuda:1', dtype=torch.int64))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.where(torch.isnan(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 0], device='cuda:1'),\n",
       " tensor([86, 86], device='cuda:1'),\n",
       " tensor([97, 97], device='cuda:1'),\n",
       " tensor([75, 75], device='cuda:1'),\n",
       " tensor([0, 1], device='cuda:1'))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.where(torch.isnan(a * b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 0], device='cuda:1'),\n",
       " tensor([97, 97], device='cuda:1'),\n",
       " tensor([75, 75], device='cuda:1'),\n",
       " tensor([0, 1], device='cuda:1'))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.where(torch.isnan(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([6, 6], device='cuda:1'),\n",
       " tensor([0, 1], device='cuda:1'),\n",
       " tensor([100, 100], device='cuda:1'),\n",
       " tensor([124, 124], device='cuda:1'))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.where(torch.isnan(warped_grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([nan], device='cuda:1', grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im1_warp[torch.isnan(im1_warp)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
